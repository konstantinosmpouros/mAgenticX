{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f594c1ba-ca36-46e8-8789-8df62851f3c7",
   "metadata": {},
   "source": [
    "# OrthodoxAI - Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd36b1-71ca-4bd8-9e69-99206f25b0e4",
   "metadata": {},
   "source": [
    "> Konstantinos Mpouros <br>\n",
    "> Github: https://github.com/konstantinosmpouros?tab=repositories<br>\n",
    "> Year: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e8053-ccff-461b-9a96-8f44526620b1",
   "metadata": {},
   "source": [
    "## About the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b5c23-4989-4a01-a419-bfb4628abcba",
   "metadata": {},
   "source": [
    "This notebook is dedicated to prototyping and testing the behavior of individual and collaborative agents within the **OrthodoxAI** multi-agent system.\n",
    "\n",
    "It serves as a sandbox to:\n",
    "\n",
    "- Define agent roles (e.g., Reflection, Generator, Summarizer)\n",
    "- Simulate agent communication structure patters\n",
    "- Experiment with routing logic and agent chaining (without LangGraph)\n",
    "- Test prompt templates and output formatting for each agent\n",
    "- Evaluate performance and consistency across multiple LLM-backed tasks\n",
    "\n",
    "This notebook is used for isolated debugging and experimentation before integrating agents into the main production pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d713835e-2b12-4806-b6c9-e771fed71705",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f3edff7-c5f4-475b-93fa-94ab5882f936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:03:11.445871Z",
     "start_time": "2025-04-16T07:03:11.434871Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the API Keys\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b8e99c-6ea1-4992-a245-a1aae18bc630",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## OpenAI APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e6840-8369-42bb-b8d5-d898ef9bbfe7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Client Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbd1076-2f94-49d4-8e08-ccbe70da8fd4",
   "metadata": {},
   "source": [
    "* Import libraries to chat with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "551f9895-c065-4ddf-8390-a248bac20927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc06a6-cd92-42b1-a0c3-93a71003f565",
   "metadata": {},
   "source": [
    "* Define a Structured Output template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f85a9871-35f4-44d1-b215-7618bd4d86ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyzerOutput(BaseModel):\n",
    "    classification: Literal[\"Religious\", \"Non-Religious\"] = Field(..., description=\"Either 'Religious' or 'Non-Religious'\")\n",
    "    key_topics: List[str] = Field(..., description=\"List of key topics/areas related to the user's question (e.g., theology, jesus, humility, virtues)\",)\n",
    "    context_requirements: str = Field(..., description=\"A clear explanation of the query's context needs\")\n",
    "    query_complexity: Literal[\"Low\", \"Medium\", \"High\"] = Field(..., description=\"'Low', 'Medium', or 'High' complexity\")\n",
    "    reasoning: str = Field(..., description=\"The Chain of Thought that has been done in order to analyze the user query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ecee3e-adc7-49a8-b31e-b4a70ae0d995",
   "metadata": {},
   "source": [
    "* Initialize configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f01b20ef-1bac-48d3-87b7-ba82518909ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'o3-mini'\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "chat_template = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant that classifies user queries as either religious or non-religious and extracts key topics.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Helloo!!\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df3561-9013-418f-8a31-448bfc45bb4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Simple response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af41ee06-48d0-47dc-b860-4463cd0773e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: Non-religious\n",
      "Key Topics: Greeting\n",
      "\n",
      "The query \"Helloo!!\" appears to be a friendly greeting with no religious content.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME, \n",
    "    messages=chat_template,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b1e8da6-c71b-4d70-9f65-c5cdc649abf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: Non-religious\n",
      "\n",
      "Key Topic:\n",
      "• Greeting\n",
      "\n",
      "The user’s message appears to be a simple, informal greeting without any religious context.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=MODEL_NAME,\n",
    "    input=chat_template\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4541d-fe4c-409a-b7d1-372ed5cdfd8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f5344f-dc43-44ce-97ce-fb44b1ff8307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: Non-religious\n",
      "\n",
      "Key Topics: Greeting\n",
      "\n",
      "The message appears to be a friendly, informal greeting with no religious content."
     ]
    }
   ],
   "source": [
    "response = \"\"\n",
    "\n",
    "for chunk in client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=chat_template,\n",
    "    stream=True\n",
    "):\n",
    "    \n",
    "    if not chunk.choices:\n",
    "        continue  # Just a safeguard\n",
    "    \n",
    "    # The partial text is in 'delta.content'\n",
    "    chunk_text = chunk.choices[0].delta.content\n",
    "    if chunk_text is None:\n",
    "        continue\n",
    "\n",
    "    response += chunk_text\n",
    "    print(chunk_text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "115f69cc-b56e-4b7b-a606-6e35befe5496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: Non-religious\n",
      "\n",
      "Key Topics: Greeting, informal salutation."
     ]
    }
   ],
   "source": [
    "stream = client.responses.create(\n",
    "    model=MODEL_NAME,\n",
    "    input=chat_template,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for event in stream:\n",
    "    if event.type == 'response.refusal.delta':\n",
    "        print(event.delta, end=\"\")\n",
    "    elif event.type == 'response.output_text.delta':\n",
    "        print(event.delta, end=\"\")\n",
    "    elif event.type == 'response.error':\n",
    "        print(event.error, end=\"\")\n",
    "    elif event.type == 'response.completed':\n",
    "        response = event.response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58ed8558-2b84-4dad-a86a-e722b8369fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseReasoningItem(id='rs_67eefa37c2048191b2860f006b1d9261003da8076e6768b6', summary=[], type='reasoning', status=None),\n",
       " ResponseOutputMessage(id='msg_67eefa381b208191931e99bd52493d89003da8076e6768b6', content=[ResponseOutputText(annotations=[], text='Classification: Non-religious\\n\\nKey Topics: Greeting, informal salutation.', type='output_text')], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654bc96-f254-4c71-a12e-e3e1600e049d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Structured response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8847254-15cd-4ee6-84f8-8b05c16e8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.beta.chat.completions.parse(\n",
    "    model=MODEL_NAME,\n",
    "    messages=chat_template,\n",
    "    response_format=AnalyzerOutput\n",
    ")\n",
    "\n",
    "structured_object = response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d149aba4-67ce-4565-9e6a-e14c2d80bf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(classification='Non-Religious', key_topics=['greeting'], context_requirements='No complexity is involved and there is no religious context present in the query. The query is simply a casual greeting.', query_complexity='Low', reasoning=\"The query 'Helloo!!' is a casual greeting with no mention of religious topics, symbols, or language. There is no indication of the conversation touching on religious themes, hence it is classified as non-religious and of low complexity.\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da3d419d-77cf-4978-b991-2f1822d5dd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"classification\": \"Non-Religious\",\\n  \"key_topics\": [\\n    \"greeting\"\\n  ],\\n  \"context_requirements\": \"No complexity is involved and there is no religious context present in the query. The query is simply a casual greeting.\",\\n  \"query_complexity\": \"Low\",\\n  \"reasoning\": \"The query \\'Helloo!!\\' is a casual greeting with no mention of religious topics, symbols, or language. There is no indication of the conversation touching on religious themes, hence it is classified as non-religious and of low complexity.\"\\n}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fab4decd-6228-4d30-bb82-5a68537989c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "assistant\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.function_call)\n",
    "print(response.choices[0].message.tool_calls)\n",
    "print(response.choices[0].message.role)\n",
    "print(response.choices[0].message.refusal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "177fa1c9-4953-4dab-a5ac-a80a4d865546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4511/789635851.py:1: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  schema_dict = AnalyzerOutput.schema()\n"
     ]
    }
   ],
   "source": [
    "schema_dict = AnalyzerOutput.schema()\n",
    "schema_dict[\"additionalProperties\"] = False\n",
    "text_dict = {\n",
    "    \"format\": {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"name\": \"MyAnalyzerOutput\",  # arbitrary name\n",
    "        \"strict\": True,\n",
    "        \"schema\": schema_dict\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=MODEL_NAME,\n",
    "    input=chat_template,\n",
    "    text=text_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "653f33e6-dfb9-415f-a903-42979b5e6d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_67eefa3c1c608191a232cc4cc3cc9aa505b562a8227eead7', created_at=1743714876.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='o3-mini-2025-01-31', object='response', output=[ResponseReasoningItem(id='rs_67eefa3e69908191a96199dec3343ce505b562a8227eead7', summary=[], type='reasoning', status=None), ResponseOutputMessage(id='msg_67eefa3f67788191b6a07580c38dfaf905b562a8227eead7', content=[ResponseOutputText(annotations=[], text='{\\n  \"classification\": \"Non-Religious\",\\n  \"key_topics\": [\\n    \"greeting\"\\n  ],\\n  \"context_requirements\": \"The query is simply a casual greeting without any religious content or additional context.\",\\n  \"query_complexity\": \"Low\",\\n  \"reasoning\": \"The user message \\'Helloo!!\\' is a greeting and does not contain any content related to religious topics. Therefore, it is classified as non-religious with the key topic being a greeting.\"\\n}', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort='medium', generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatTextJSONSchemaConfig(name='MyAnalyzerOutput', schema_={'properties': {'classification': {'description': \"Either 'Religious' or 'Non-Religious'\", 'enum': ['Religious', 'Non-Religious'], 'title': 'Classification', 'type': 'string'}, 'key_topics': {'description': \"List of key topics/areas related to the user's question (e.g., theology, jesus, humility, virtues)\", 'items': {'type': 'string'}, 'title': 'Key Topics', 'type': 'array'}, 'context_requirements': {'description': \"A clear explanation of the query's context needs\", 'title': 'Context Requirements', 'type': 'string'}, 'query_complexity': {'description': \"'Low', 'Medium', or 'High' complexity\", 'enum': ['Low', 'Medium', 'High'], 'title': 'Query Complexity', 'type': 'string'}, 'reasoning': {'description': 'The Chain of Thought that has been done in order to analyze the user query', 'title': 'Reasoning', 'type': 'string'}}, 'required': ['classification', 'key_topics', 'context_requirements', 'query_complexity', 'reasoning'], 'title': 'AnalyzerOutput', 'type': 'object', 'additionalProperties': False}, type='json_schema', description=None, strict=True)), truncation='disabled', usage=ResponseUsage(input_tokens=36, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=423, output_tokens_details=OutputTokensDetails(reasoning_tokens=320), total_tokens=459), user=None, store=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "024a7455-55d7-4d79-8cd0-0b3a442a53d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"classification\": \"Non-Religious\",\\n  \"key_topics\": [\\n    \"greeting\"\\n  ],\\n  \"context_requirements\": \"The query is simply a casual greeting without any religious content or additional context.\",\\n  \"query_complexity\": \"Low\",\\n  \"reasoning\": \"The user message \\'Helloo!!\\' is a greeting and does not contain any content related to religious topics. Therefore, it is classified as non-religious with the key topic being a greeting.\"\\n}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56e0e4a8-ef99-43bd-802e-7c33047bfd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseOutputText(annotations=[], text='{\\n  \"classification\": \"Non-Religious\",\\n  \"key_topics\": [\\n    \"greeting\"\\n  ],\\n  \"context_requirements\": \"The query is simply a casual greeting without any religious content or additional context.\",\\n  \"query_complexity\": \"Low\",\\n  \"reasoning\": \"The user message \\'Helloo!!\\' is a greeting and does not contain any content related to religious topics. Therefore, it is classified as non-religious with the key topic being a greeting.\"\\n}', type='output_text')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output[1].content[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a8341-3e6a-49e0-8132-ff89f9bd013c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Multi-Step Structured Response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d5da65-0d59-4bf3-b8b4-8dca74ecd863",
   "metadata": {},
   "source": [
    "* Initialize the main template and the step's template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff2cc0c8-b610-4b71-a904-e15cca88be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5b53ab7-5af6-41b9-8fa4-8cac66d0b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n",
    "    ],\n",
    "    response_format=MathReasoning,\n",
    ")\n",
    "\n",
    "math_reasoning = completion.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a86bea6-3f00-4e0e-ae3b-40758b17a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_reasoning.refusal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de96d67d-7a44-4f1a-b023-786f1f50cbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MathReasoning(steps=[Step(explanation=\"The equation given is 8x + 7 = -23. We want to isolate the variable x. To do this, start by removing the constant term on the left side of the equation, which is 7. We'll do this by subtracting 7 from both sides of the equation.\", output='8x + 7 - 7 = -23 - 7'), Step(explanation='Subtracting 7 from both sides simplifies the equation. The left side becomes 8x, and the right side becomes -23 - 7, which is -30.', output='8x = -30'), Step(explanation='Now, we need to solve for x by isolating it. Since x is multiplied by 8, we do the opposite operation, which is dividing by 8, on both sides of the equation.', output='x = -30 / 8'), Step(explanation='Simplify the right side of the equation by performing the division. -30 divided by 8 simplifies to -3.75.', output='x = -3.75')], final_answer='x = -3.75')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_reasoning.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "691a9dbd-f520-46c7-87af-7e0692718cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x = -3.75'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_reasoning.parsed.final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49cead43-2452-412b-9276-0928e6a22ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Step(explanation=\"The equation given is 8x + 7 = -23. We want to isolate the variable x. To do this, start by removing the constant term on the left side of the equation, which is 7. We'll do this by subtracting 7 from both sides of the equation.\", output='8x + 7 - 7 = -23 - 7'),\n",
       " Step(explanation='Subtracting 7 from both sides simplifies the equation. The left side becomes 8x, and the right side becomes -23 - 7, which is -30.', output='8x = -30'),\n",
       " Step(explanation='Now, we need to solve for x by isolating it. Since x is multiplied by 8, we do the opposite operation, which is dividing by 8, on both sides of the equation.', output='x = -30 / 8'),\n",
       " Step(explanation='Simplify the right side of the equation by performing the division. -30 divided by 8 simplifies to -3.75.', output='x = -3.75')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_reasoning.parsed.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0556a1-d37a-4656-9149-dd13d9347b81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Streaming Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c601a5d8-100d-4c91-8ba2-82a9e42c9a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4511/1884170164.py:1: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  schema_dict = AnalyzerOutput.schema()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'properties': {'classification': {'description': \"Either 'Religious' or 'Non-Religious'\",\n",
       "   'enum': ['Religious', 'Non-Religious'],\n",
       "   'title': 'Classification',\n",
       "   'type': 'string'},\n",
       "  'key_topics': {'description': \"List of key topics/areas related to the user's question (e.g., theology, jesus, humility, virtues)\",\n",
       "   'items': {'type': 'string'},\n",
       "   'title': 'Key Topics',\n",
       "   'type': 'array'},\n",
       "  'context_requirements': {'description': \"A clear explanation of the query's context needs\",\n",
       "   'title': 'Context Requirements',\n",
       "   'type': 'string'},\n",
       "  'query_complexity': {'description': \"'Low', 'Medium', or 'High' complexity\",\n",
       "   'enum': ['Low', 'Medium', 'High'],\n",
       "   'title': 'Query Complexity',\n",
       "   'type': 'string'},\n",
       "  'reasoning': {'description': 'The Chain of Thought that has been done in order to analyze the user query',\n",
       "   'title': 'Reasoning',\n",
       "   'type': 'string'}},\n",
       " 'required': ['classification',\n",
       "  'key_topics',\n",
       "  'context_requirements',\n",
       "  'query_complexity',\n",
       "  'reasoning'],\n",
       " 'title': 'AnalyzerOutput',\n",
       " 'type': 'object',\n",
       " 'additionalProperties': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_dict = AnalyzerOutput.schema()\n",
    "schema_dict[\"additionalProperties\"] = False\n",
    "schema_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a390035-dbc1-45e8-9126-2f0385c5df4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'format': {'type': 'json_schema',\n",
       "  'name': 'MyAnalyzerOutput',\n",
       "  'schema': {'properties': {'classification': {'description': \"Either 'Religious' or 'Non-Religious'\",\n",
       "     'enum': ['Religious', 'Non-Religious'],\n",
       "     'title': 'Classification',\n",
       "     'type': 'string'},\n",
       "    'key_topics': {'description': \"List of key topics/areas related to the user's question (e.g., theology, jesus, humility, virtues)\",\n",
       "     'items': {'type': 'string'},\n",
       "     'title': 'Key Topics',\n",
       "     'type': 'array'},\n",
       "    'context_requirements': {'description': \"A clear explanation of the query's context needs\",\n",
       "     'title': 'Context Requirements',\n",
       "     'type': 'string'},\n",
       "    'query_complexity': {'description': \"'Low', 'Medium', or 'High' complexity\",\n",
       "     'enum': ['Low', 'Medium', 'High'],\n",
       "     'title': 'Query Complexity',\n",
       "     'type': 'string'},\n",
       "    'reasoning': {'description': 'The Chain of Thought that has been done in order to analyze the user query',\n",
       "     'title': 'Reasoning',\n",
       "     'type': 'string'}},\n",
       "   'required': ['classification',\n",
       "    'key_topics',\n",
       "    'context_requirements',\n",
       "    'query_complexity',\n",
       "    'reasoning'],\n",
       "   'title': 'AnalyzerOutput',\n",
       "   'type': 'object',\n",
       "   'additionalProperties': False}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict = {\n",
    "    \"format\": {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"name\": \"MyAnalyzerOutput\",  # arbitrary name\n",
    "        \"schema\": schema_dict\n",
    "    }\n",
    "}\n",
    "text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c74ceafa-f617-4147-8b98-e48d99c9cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"classification\": \"Non-Religious\",\n",
      "  \"key_topics\": [\"greeting\"],\n",
      "  \"context_requirements\": \"The query is a simple greeting message and does not imply any religious context.\",\n",
      "  \"query_complexity\": \"Low\",\n",
      "  \"reasoning\": \"The user's message 'Helloo!!' is a casual greeting without any religious elements. It falls under non-religious content, and the simplicity of the greeting means that the query is of low complexity.\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "stream = client.responses.create(\n",
    "    model=MODEL_NAME,\n",
    "    input=chat_template,\n",
    "    text=text_dict,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for event in stream:\n",
    "    if event.type == 'response.refusal.delta':\n",
    "        print(event.delta, end=\"\")\n",
    "    elif event.type == 'response.output_text.delta':\n",
    "        print(event.delta, end=\"\")\n",
    "    elif event.type == 'response.error':\n",
    "        print(event.error, end=\"\")\n",
    "    elif event.type == 'response.completed':\n",
    "        response = event.response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3bf56d0-8171-4eef-8dce-86d2e8ae140f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseReasoningItem(id='rs_67eefa4a0c588191a0db7699d16846980f551bba5511bb39', summary=[], type='reasoning', status=None),\n",
       " ResponseOutputMessage(id='msg_67eefa4acbec8191bedd9651ad84bf120f551bba5511bb39', content=[ResponseOutputText(annotations=[], text='{\\n  \"classification\": \"Non-Religious\",\\n  \"key_topics\": [\"greeting\"],\\n  \"context_requirements\": \"The query is a simple greeting message and does not imply any religious context.\",\\n  \"query_complexity\": \"Low\",\\n  \"reasoning\": \"The user\\'s message \\'Helloo!!\\' is a casual greeting without any religious elements. It falls under non-religious content, and the simplicity of the greeting means that the query is of low complexity.\"\\n}', type='output_text')], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee181dc1-467c-43b0-8eeb-b6343b9a17e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseReasoningItem(id='rs_67eefa4a0c588191a0db7699d16846980f551bba5511bb39', summary=[], type='reasoning', status=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6c17a89-fa55-4967-9943-6683422c2dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseOutputMessage(id='msg_67eefa4acbec8191bedd9651ad84bf120f551bba5511bb39', content=[ResponseOutputText(annotations=[], text='{\\n  \"classification\": \"Non-Religious\",\\n  \"key_topics\": [\"greeting\"],\\n  \"context_requirements\": \"The query is a simple greeting message and does not imply any religious context.\",\\n  \"query_complexity\": \"Low\",\\n  \"reasoning\": \"The user\\'s message \\'Helloo!!\\' is a casual greeting without any religious elements. It falls under non-religious content, and the simplicity of the greeting means that the query is of low complexity.\"\\n}', type='output_text')], role='assistant', status='completed', type='message')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fff3f5ad-e1d0-4352-95c0-4217e948e298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'assistant'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[1].role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b2d3fb2-d046-42f9-aa24-7fb13c9b2acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'completed'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[1].status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d51a2ac-8476-42b2-9c9e-e7a0c58ab15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"classification\": \"Non-Religious\",\\n  \"key_topics\": [\"greeting\"],\\n  \"context_requirements\": \"The query is a simple greeting message and does not imply any religious context.\",\\n  \"query_complexity\": \"Low\",\\n  \"reasoning\": \"The user\\'s message \\'Helloo!!\\' is a casual greeting without any religious elements. It falls under non-religious content, and the simplicity of the greeting means that the query is of low complexity.\"\\n}'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[1].content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac5aa6-2fca-466b-ba02-e7a19cc67ff6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Function Calling Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6b3d08e-fbe7-432c-9a01-39b01476a14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get current temperature for a given location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"City and country e.g. Bogotá, Colombia\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"location\"\n",
    "        ],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "707735f0-5fb3-4e41-bf61-4f11c242fafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_67eefa4c1c1c81918cf2ded63d178ee201399e5e5dcbafd3', created_at=1743714892.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='o3-mini-2025-01-31', object='response', output=[ResponseReasoningItem(id='rs_67eefa4cffb48191a0ad2433af706f7f01399e5e5dcbafd3', summary=[], type='reasoning', status=None), ResponseFunctionToolCall(arguments='{\"location\": \"Paris, France\"}', call_id='call_StVkKwib4bjRUbJtum2aip1G', name='get_weather', type='function_call', id='fc_67eefa4d79208191918dac892115886a01399e5e5dcbafd3', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='get_weather', parameters={'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'City and country e.g. Bogotá, Colombia'}}, 'required': ['location'], 'additionalProperties': False}, strict=True, type='function', description='Get current temperature for a given location.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort='medium', generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=68, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=79, output_tokens_details=OutputTokensDetails(reasoning_tokens=64), total_tokens=147), user=None, store=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=MODEL_NAME,\n",
    "    input=[{\"role\": \"user\", \"content\": \"What is the weather like in Paris today?\"}],\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3ea1eb3-3bb6-4cfc-b853-96e8dd33fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"location\": \"Paris, France\"}\n",
      "fc_67eefa4d79208191918dac892115886a01399e5e5dcbafd3\n",
      "function_call\n",
      "get_weather\n"
     ]
    }
   ],
   "source": [
    "print(response.output[1].arguments)\n",
    "print(response.output[1].id)\n",
    "print(response.output[1].type)\n",
    "print(response.output[1].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d1e56-5f44-44f9-be10-163b705fb49d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Langchain LLM-Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9815d4fd-cf6a-42c5-a725-5e44588c9776",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Langchain Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3005a-f51e-46c3-a4dd-714db9780330",
   "metadata": {},
   "source": [
    "* Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d8e3be1-c962-4c26-94da-b17f9614b798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:03:26.262067Z",
     "start_time": "2025-04-16T07:03:25.330083Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de0eab-146f-472d-8e49-4b28607e85ce",
   "metadata": {},
   "source": [
    "* Initialize Structure Output template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe51cb6-2a8b-4674-b2fb-70ae9485e981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:03:26.772200Z",
     "start_time": "2025-04-16T07:03:26.760202Z"
    }
   },
   "outputs": [],
   "source": [
    "class AnalyzerOutput(BaseModel):\n",
    "    classification: Literal[\"Religious\", \"Non-Religious\"] = Field(..., description=\"Either 'Religious' or 'Non-Religious'\")\n",
    "    key_topics: List[str] = Field(..., description=\"List of key topics/areas related to the user's question (e.g., theology, jesus, humility, virtues)\",)\n",
    "    context_requirements: str = Field(..., description=\"A clear explanation of the query's context needs\")\n",
    "    query_complexity: Literal[\"Low\", \"Medium\", \"High\"] = Field(..., description=\"'Low', 'Medium', or 'High' complexity\")\n",
    "    reasoning: str = Field(..., description=\"The Chain of Thought that has been done in order to analyze the user query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb277a-24e3-4b68-bc13-c8ae97fe2cd0",
   "metadata": {},
   "source": [
    "* Initialize configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03468a9e-0573-4c74-9e5c-99ec4fa411a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:03:29.165965Z",
     "start_time": "2025-04-16T07:03:29.153965Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'o3-mini'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ca3cd-1d5b-4842-b59e-cf4c410d03ee",
   "metadata": {},
   "source": [
    "* Initialize prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384cd3ee-af74-4dd9-a466-9535575be0c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:03:30.579537Z",
     "start_time": "2025-04-16T07:03:30.561229Z"
    }
   },
   "outputs": [],
   "source": [
    "analyzer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an AI assistant that classifies user queries as either religious or non-religious and extracts key topics.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Classify the following query and extract key concepts:\\n\\nQuery: {query}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant named {assistant_name}. Use the available tools to provide accurate information.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{user_input}\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c2c40-cabe-4dce-a754-1f1a907488ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chain with Simple Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41f9c072-bb06-41b0-a103-018928ea1d1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:34:48.336273Z",
     "start_time": "2025-04-16T06:34:48.320270Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=MODEL_NAME)\n",
    "analyzer_chain = analyzer_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a934264-dc7d-48f2-8d40-d96c0f67bfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Classification: Non-religious\\n\\nKey Concepts:\\n• Greeting\\n• Salutation\\n• Casual communication', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 46, 'total_tokens': 141, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_617f206dd9', 'finish_reason': 'stop', 'logprobs': None}, id='run-490abbfa-d40a-4703-9ec2-8adba68e51f3-0', usage_metadata={'input_tokens': 46, 'output_tokens': 95, 'total_tokens': 141, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = analyzer_chain.invoke('Hello')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ab1aefe-e570-49f7-8fa9-a7a1eb6447bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: Non-religious\n",
      "\n",
      "Key Concepts:\n",
      "• Greeting\n",
      "• Salutation\n",
      "• Casual communication \n",
      "\n",
      "\n",
      "{'token_usage': {'completion_tokens': 95, 'prompt_tokens': 46, 'total_tokens': 141, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_617f206dd9', 'finish_reason': 'stop', 'logprobs': None} \n",
      "\n",
      "\n",
      "run-490abbfa-d40a-4703-9ec2-8adba68e51f3-0 \n",
      "\n",
      "\n",
      "{'input_tokens': 46, 'output_tokens': 95, 'total_tokens': 141, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}}\n"
     ]
    }
   ],
   "source": [
    "print(text.content, '\\n\\n')\n",
    "print(text.response_metadata, '\\n\\n')\n",
    "print(text.id, '\\n\\n')\n",
    "print(text.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944ffa9-e6af-4b1c-9d01-635950bcdabe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chain with Simple Response & Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "877a5ab3-a601-4e17-b496-c0292c35f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Fetches the current weather for a specified location.\"\"\"\n",
    "    # Placeholder implementation\n",
    "    return f\"The current weather in {location} is sunny with a temperature of 25°C.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f41bf7e-f734-4608-a594-0ac14dd33664",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=MODEL_NAME)\n",
    "llm_with_tools = llm.bind_tools([get_weather])\n",
    "chain = chat_prompt | llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bf12e66-23d6-4013-b5f3-fd970724160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"assistant_name\": \"Athena\",\n",
    "    \"user_input\": \"What's the weather in Thessaloniki?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2df8ccf-4bad-499b-a2f8-9a5f0ebe0c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_4MpDWsYBZjvyzHFfozCteWJK', 'function': {'arguments': '{\"location\": \"Thessaloniki\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 75, 'total_tokens': 101, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_c87fd29531', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1c8d2e06-c3bc-48af-84a3-59a1a95d4d18-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'Thessaloniki'}, 'id': 'call_4MpDWsYBZjvyzHFfozCteWJK', 'type': 'tool_call'}], usage_metadata={'input_tokens': 75, 'output_tokens': 26, 'total_tokens': 101, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3de01b6e-12df-4669-b972-6c065e78ec86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_weather',\n",
       "  'args': {'location': 'Thessaloniki'},\n",
       "  'id': 'call_4MpDWsYBZjvyzHFfozCteWJK',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36e75e76-5cce-4b63-82ab-4e468eed46d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'get_weather',\n",
       " 'args': {'location': 'Thessaloniki'},\n",
       " 'id': 'call_4MpDWsYBZjvyzHFfozCteWJK',\n",
       " 'type': 'tool_call'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65126128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(getattr(response, \"tool_calls\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d903491e-0ef4-48d0-9175-f2bd0680e0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'get_weather',\n",
       " 'args': {'location': 'Thessaloniki'},\n",
       " 'id': 'call_4MpDWsYBZjvyzHFfozCteWJK',\n",
       " 'type': 'tool_call'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f4f0522-431f-403d-8d88-706046e0d4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51c9913c-4610-400f-aada-a20b8cff6608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tool_calls'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata['finish_reason']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79a43f15-5dc1-4d2b-8fdf-858157ca25be",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"assistant_name\": \"Athena\",\n",
    "    \"user_input\": \"What's your name?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb61071f-db4a-4df5-9124-8f3bc741d235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is Athena. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 71, 'total_tokens': 159, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_99801fd0cf', 'id': 'chatcmpl-BNGA2V1BN85izKhvvsSJDqqkgv09M', 'finish_reason': 'stop', 'logprobs': None}, id='run-0d7ca203-955b-484e-b893-3459d3cbb883-0', usage_metadata={'input_tokens': 71, 'output_tokens': 88, 'total_tokens': 159, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d2a4c9b-1b7a-4b06-b4ee-0eee6ed2c559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb87deaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(getattr(response, \"tool_calls\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f558201-7e20-4998-a947-f0d2ba51eecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Athena. How can I assist you today?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b52806b6-e96b-47d2-aaa7-496dddca93a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stop'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata['finish_reason']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2a803e-2f5d-4d39-95f0-b474efed49f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chain with Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6dbe4b66-4cc2-4f28-be86-3b27561bdb0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=MODEL_NAME, streaming=True)\n",
    "analyzer_chain = analyzer_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76b0d784-c7ba-456d-b058-63800a9c5646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: Non-religious\n",
      "\n",
      "Key topics: Greeting, Salutation\n",
      "\n",
      "The query \"Hello!!\" is a casual greeting and does not contain any religious content."
     ]
    }
   ],
   "source": [
    "response = ''\n",
    "for token in analyzer_chain.stream('Hello!!'):\n",
    "    response += token.content\n",
    "    print(token.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8cf68537-5358-4293-9cda-e45d98e68137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classification: Non-religious\\n\\nKey topics: Greeting, Salutation\\n\\nThe query \"Hello!!\" is a casual greeting and does not contain any religious content.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e9893-d7ba-4724-9d4f-dcce243f8be1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chain with Streaming Response & Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b8bee014-770b-4223-b52d-eb05eb813c63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:03:37.064970Z",
     "start_time": "2025-04-16T07:03:37.054968Z"
    }
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Fetches the current weather for a specified location.\"\"\"\n",
    "    # Placeholder implementation\n",
    "    return f\"The current weather in {location} is sunny with a temperature of 25°C.\"\n",
    "\n",
    "@tool\n",
    "def get_time(location: str) -> str:\n",
    "    \"\"\"Fetches the current time for a specified location.\"\"\"\n",
    "    # Placeholder implementation\n",
    "    return f\"The current time in {location} is 10:00AM.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c74110c8-af4d-4a2f-b79f-7093d441623f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:03:40.691450Z",
     "start_time": "2025-04-16T07:03:40.672450Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=MODEL_NAME, streaming=True)\n",
    "llm = llm.bind_tools([get_weather, get_time])\n",
    "chain = chat_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "29248a6c-8ffe-41e5-98bd-e2d6b4bd03c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:03:42.215924Z",
     "start_time": "2025-04-16T07:03:42.211924Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"assistant_name\": \"Athena\",\n",
    "    \"user_input\": \"What's your name?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "22eed1a5-8926-400f-a55d-3e7af0ca64e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:03:43.519789Z",
     "start_time": "2025-04-16T07:03:43.505789Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_tool_content(chunk, tool_calls):\n",
    "    if chunk.tool_call_chunks:\n",
    "        for tool_chunk in chunk.tool_call_chunks:\n",
    "            index = tool_chunk['index']\n",
    "            if index not in tool_calls:\n",
    "                tool_calls[index] = {\n",
    "                    'name': tool_chunk['name'],  # might be None at first\n",
    "                    'args': '',\n",
    "                    'id': tool_chunk['id']\n",
    "                }\n",
    "            if tool_chunk['name'] is not None:\n",
    "                tool_calls[index]['name'] = tool_chunk['name']\n",
    "            if tool_chunk['args']:\n",
    "                tool_calls[index]['args'] += tool_chunk['args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ac2f3e70-08e8-426e-8b6d-27e1db2ded75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:03:45.914677Z",
     "start_time": "2025-04-16T07:03:44.831798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm Athena, your helpful assistant. How can I help you today?"
     ]
    }
   ],
   "source": [
    "response = ''\n",
    "tool_calls = {}\n",
    "\n",
    "for chunk in chain.stream(inputs):\n",
    "    response += chunk.content\n",
    "    print(chunk.content, end='', flush=True)\n",
    "    \n",
    "    extract_tool_content(chunk, tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "28b85c8a-bc25-420a-a322-4e2a4c9686e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "128645b2-a673-4bb3-8e65-90d81cef707e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm Athena, your helpful assistant. How can I help you today?\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5e207cab-7f38-4f26-af0a-d81104a0b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"assistant_name\": \"Athena\",\n",
    "    \"user_input\": \"What's the weather like in Athens Greece and what your name?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f67739a0-f068-49b8-8530-d7b9572920d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ''\n",
    "tool_calls = {}\n",
    "\n",
    "for chunk in chain.stream(inputs):\n",
    "    response += chunk.content\n",
    "    print(chunk.content, end='', flush=True)\n",
    "    \n",
    "    extract_tool_content(chunk, tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "545c8a7c-0aa2-45ab-8bb4-3ef0eb19feee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'name': 'get_weather',\n",
       "  'args': '{\"location\": \"Athens, Greece\"}',\n",
       "  'id': 'call_UP86gO62T8ymwQpdXJBUlApA'}}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b35ec535-87f9-4980-a4f4-02a49295f83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3fcfa754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "06ba6d0a-3692-4557-a68b-a48945c006d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'get_weather',\n",
       " 'args': '{\"location\": \"Athens, Greece\"}',\n",
       " 'id': 'call_UP86gO62T8ymwQpdXJBUlApA'}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls = tool_calls.pop(0)\n",
    "tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "73ed7ec7-25f6-43e9-9be6-3b360d27dfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_weather'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4c130dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"assistant_name\": \"Athena\",\n",
    "    \"user_input\": \"What's the weather like in Athens Greece and what your name?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "887563d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_zTTCDbWDq8vSiQZvxVDrPhAB', 'function': {'arguments': '', 'name': 'get_weather'}, 'type': 'function'}]} response_metadata={} id='run-9e493d58-206b-4d81-808d-4c0fd0e11b96' tool_calls=[{'name': 'get_weather', 'args': {}, 'id': 'call_zTTCDbWDq8vSiQZvxVDrPhAB', 'type': 'tool_call'}] tool_call_chunks=[{'name': 'get_weather', 'args': '', 'id': 'call_zTTCDbWDq8vSiQZvxVDrPhAB', 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "\n",
      "\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '{\"', 'name': None}, 'type': None}]} response_metadata={} id='run-9e493d58-206b-4d81-808d-4c0fd0e11b96' tool_calls=[{'name': '', 'args': {}, 'id': None, 'type': 'tool_call'}] tool_call_chunks=[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "\n",
      "\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'location', 'name': None}, 'type': None}]} response_metadata={} id='run-9e493d58-206b-4d81-808d-4c0fd0e11b96' invalid_tool_calls=[{'name': None, 'args': 'location', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': 'location', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "\n",
      "\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\":', 'name': None}, 'type': None}]} response_metadata={} id='run-9e493d58-206b-4d81-808d-4c0fd0e11b96' invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "\n",
      "\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' \"', 'name': None}, 'type': None}]} response_metadata={} id='run-9e493d58-206b-4d81-808d-4c0fd0e11b96' invalid_tool_calls=[{'name': None, 'args': ' \"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': ' \"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "\n",
      "\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'Ath', 'name': None}, 'type': None}]} response_metadata={} id='run-9e493d58-206b-4d81-808d-4c0fd0e11b96' invalid_tool_calls=[{'name': None, 'args': 'Ath', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': 'Ath', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "\n",
      "\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'ens', 'name': None}, 'type': None}]} response_metadata={} id='run-9e493d58-206b-4d81-808d-4c0fd0e11b96' invalid_tool_calls=[{'name': None, 'args': 'ens', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': 'ens', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "\n",
      "\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ',', 'name': None}, 'type': None}]} response_metadata={} id='run-9e493d58-206b-4d81-808d-4c0fd0e11b96' invalid_tool_calls=[{'name': None, 'args': ',', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': ',', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "\n",
      "\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' Greece', 'name': None}, 'type': None}]} response_metadata={} id='run-9e493d58-206b-4d81-808d-4c0fd0e11b96' invalid_tool_calls=[{'name': None, 'args': ' Greece', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': ' Greece', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "\n",
      "\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\"}', 'name': None}, 'type': None}]} response_metadata={} id='run-9e493d58-206b-4d81-808d-4c0fd0e11b96' invalid_tool_calls=[{'name': None, 'args': '\"}', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '\"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "\n",
      "\n",
      "\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_99801fd0cf'} id='run-9e493d58-206b-4d81-808d-4c0fd0e11b96'\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = ''\n",
    "tool_calls = {}\n",
    "\n",
    "for chunk in chain.stream(inputs):\n",
    "    print(chunk)\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    extract_tool_content(chunk, tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9ef1fbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_99801fd0cf'}, id='run-9e493d58-206b-4d81-808d-4c0fd0e11b96')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fc9cdfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "if getattr(chunk, \"content\", None):\n",
    "    print(1)\n",
    "else:\n",
    "    print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e93334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    {\"assistant_name\": \"Athena\", \"user_input\": \"What's the weather forecast for Paris France today, and what's your name?\"},\n",
    "    {\"assistant_name\": \"Athena\", \"user_input\": \"Can you tell me the current weather in Tokyo Japan and also your name?\"},\n",
    "    {\"assistant_name\": \"Athena\", \"user_input\": \"How's the weather in Sydney Australia right now, and what do you go by?\"},\n",
    "    {\"assistant_name\": \"Athena\", \"user_input\": \"Could you check the weather in New York City and remind me of your name?\"},\n",
    "    {\"assistant_name\": \"Athena\", \"user_input\": \"What's the temperature like in Berlin Germany today, and what are you called?\"},\n",
    "    {\"assistant_name\": \"Athena\", \"user_input\": \"Can you update me on the weather in Dubai UAE and state your name?\"},\n",
    "    {\"assistant_name\": \"Athena\", \"user_input\": \"What's the climate like in Toronto Canada at this moment, and what's your name?\"},\n",
    "    {\"assistant_name\": \"Athena\", \"user_input\": \"How is the weather in Rome Italy this weekend, and may I know your name?\"},\n",
    "    {\"assistant_name\": \"Athena\", \"user_input\": \"Could you describe the weather conditions in Madrid Spain and share your name?\"},\n",
    "    {\"assistant_name\": \"Athena\", \"user_input\": \"What's the weather outlook for London UK tomorrow, and what's your name again?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6d53d46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'name': 'get_weather', 'args': '{\"location\": \"Paris, France\"}', 'id': 'call_FKhaZXqSrtkaHs7XUjB1fSnL'}}\n",
      "{0: {'name': 'get_weather', 'args': '{\"location\": \"Tokyo, Japan\"}', 'id': 'call_HAXd69fHpmAFljPdb6K8qdJh'}}\n",
      "{0: {'name': 'get_weather', 'args': '{\"location\": \"Sydney, Australia\"}', 'id': 'call_c8hEyT42fc2YSgzov0bgrpAk'}}\n",
      "{0: {'name': 'get_weather', 'args': '{\"location\": \"New York City\"}', 'id': 'call_3osuInWOvPZEcblwiyJOspLs'}}\n",
      "{0: {'name': 'get_weather', 'args': '{\"location\": \"Berlin, Germany\"}', 'id': 'call_cmwW92qnzjowRJvA15mQZyUi'}}\n",
      "{0: {'name': 'get_weather', 'args': '{\"location\": \"Dubai, UAE\"}', 'id': 'call_sfsKbUpNS4GorxNoOAQFbrrL'}}\n",
      "{0: {'name': 'get_weather', 'args': '{\"location\": \"Toronto, Canada\"}', 'id': 'call_eyQ0Okmxy2a8YjbpcmFTahfh'}}\n",
      "{0: {'name': 'get_weather', 'args': '{\"location\": \"Rome, Italy\"}', 'id': 'call_LkJ7CFjbdb23Avkw5BAW4FBx'}}\n",
      "{0: {'name': 'get_weather', 'args': '{\"location\": \"Madrid, Spain\"}', 'id': 'call_hZahEoWBFwJK5m8pco148sLA'}}\n",
      "{0: {'name': 'get_weather', 'args': '{\"location\": \"London UK\"}', 'id': 'call_M1BjcshreT5RWNI7HDP1yaKn'}}\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    response = ''\n",
    "    tool_calls = {}\n",
    "\n",
    "    for chunk in chain.stream(question):\n",
    "        response += chunk.content\n",
    "        print(chunk.content, end='', flush=True)\n",
    "        \n",
    "        extract_tool_content(chunk, tool_calls)\n",
    "    print(tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "77982622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'get_weather', 'args': '{\"location\": \"London UK\"}', 'id': 'call_M1BjcshreT5RWNI7HDP1yaKn'}\n"
     ]
    }
   ],
   "source": [
    "for call in tool_calls.values():\n",
    "    print(call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb350d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-2024-08-06\", streaming=True)\n",
    "llm = llm.bind_tools([get_weather, get_time])\n",
    "llm = llm.bind(parallel_tool_calls=True)\n",
    "chain = chat_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e91151",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"assistant_name\": \"Athena\",\n",
    "    \"user_input\": \"What's the weather and time like in Athens Greece?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ''\n",
    "tool_calls = {}\n",
    "\n",
    "for chunk in chain.stream(inputs):\n",
    "    response += chunk.content\n",
    "    print(chunk.content, end='', flush=True)\n",
    "    \n",
    "    extract_tool_content(chunk, tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4af62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'name': 'get_weather',\n",
       "  'args': '{\"location\": \"Athens, Greece\"}',\n",
       "  'id': 'call_nJz1gNxWfFLZpc9qoA4yFjEc'},\n",
       " 1: {'name': 'get_time',\n",
       "  'args': '{\"location\": \"Athens, Greece\"}',\n",
       "  'id': 'call_LP6nKCFCJGwF43lzXeJJlB1n'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca86bac-a959-4c12-adb3-b69a6ca2eafe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chain with Structure Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d80e9f3f-db3c-4dae-9fce-5e532be48d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_llm = ChatOpenAI(model=MODEL_NAME).with_structured_output(AnalyzerOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "823d487f-ec8d-41c8-8132-ac5a8515bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_chain = analyzer_prompt | analyzer_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a90cc8ce-3ffe-4c4b-be51-162eecb4eb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(classification='Non-Religious', key_topics=['Greetings', 'Salutation'], context_requirements=\"The query 'Hello' is a simple greeting with no additional context provided, and it does not contain religious themes or specific topics. More context would be needed if the query intended to delve into religious or complex topics.\", query_complexity='Low', reasoning=\"The query 'Hello' is a basic greeting and does not include any religious themes, practices, or terminology. It is classified as non-religious and is considered a low complexity query involving common conversational language.\")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = analyzer_chain.invoke('Hello')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2d575e64-63b6-45cd-b4b3-70e2a78211be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Religious \n",
      "\n",
      "\n",
      "The query 'Hello' is a simple greeting with no additional context provided, and it does not contain religious themes or specific topics. More context would be needed if the query intended to delve into religious or complex topics. \n",
      "\n",
      "\n",
      "['Greetings', 'Salutation'] \n",
      "\n",
      "\n",
      "Low \n",
      "\n",
      "\n",
      "The query 'Hello' is a basic greeting and does not include any religious themes, practices, or terminology. It is classified as non-religious and is considered a low complexity query involving common conversational language. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text.classification, '\\n\\n')\n",
    "print(text.context_requirements, '\\n\\n')\n",
    "print(text.key_topics, '\\n\\n')\n",
    "print(text.query_complexity, '\\n\\n')\n",
    "print(text.reasoning, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db5bc60a-d440-4e68-a981-260438ca96d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.AnalyzerOutput"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af5453d9-f5b2-4811-91a7-cb7c6ba9e0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query 'Hello' is a basic greeting and does not include any religious themes, practices, or terminology. It is classified as non-religious and is considered a low complexity query involving common conversational language.\n",
      "\n",
      "The analysis of the user query shows that the question is Non-Religious and with a Low complexity. Specifically, the analysis said the following: The query 'Hello' is a simple greeting with no additional context provided, and it does not contain religious themes or specific topics. More context would be needed if the query intended to delve into religious or complex topics. Key topics: Greetings, Salutation.\n"
     ]
    }
   ],
   "source": [
    "analysis_report = (\n",
    "    f\"{text.reasoning}\\n\\n\"\n",
    "    f\"The analysis of the user query shows that the question is {text.classification} \"\n",
    "    f\"and with a {text.query_complexity} complexity. Specifically, the analysis said the following: \"\n",
    "    f\"{text.context_requirements} Key topics: {', '.join(text.key_topics)}.\"\n",
    ")\n",
    "print(analysis_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb284e-9314-45aa-9c80-6067956f7544",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Langchain Agentic Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a963b298-0900-4f48-81e4-711b2b298024",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Summarizer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20d9c846-8f0c-4853-b220-042d79c6f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "REASONING_LLM_NAME = \"o3-mini\"\n",
    "LLM_NAME = \"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e9f6c31-6dca-47d1-b54d-130cfcd01643",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an AI assistant called Summarization Agent that takes multiple retrieved text chunks related to a user query \"\n",
    "            \"and generates a concise and informative summary. \"\n",
    "            \"Your goal is to synthesize the content, extract key themes, facts, and insights, and present them in a well-structured summary along with citations if possible. \"\n",
    "            \"Avoid repetition, remove irrelevant details, and prioritize clarity, coherence, and theological accuracy when applicable.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the list of retrieved contents:\\n\\n{retrieved_chunks}\\n\\n\"\n",
    "            \"Summarize the content in a clear, structured way. Highlight key points, theological concepts, and any important distinctions. \"\n",
    "            \"Make sure the summary is helpful for answering the user's original question.\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6e7411d-e1fa-4d43-b3cb-bb91345427c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- The Orthodox Church believes that icons are a window to the divine and play a vital role in worship.\\n\\n- Icons are venerated, not worshipped, and serve as tools for teaching and spiritual reflection.\\n\\n- The Seventh Ecumenical Council affirmed the use of icons in 787 AD, emphasizing their doctrinal importance.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_chunks = [\n",
    "    \"The Orthodox Church believes that icons are a window to the divine and play a vital role in worship.\",\n",
    "    \"Icons are venerated, not worshipped, and serve as tools for teaching and spiritual reflection.\",\n",
    "    \"The Seventh Ecumenical Council affirmed the use of icons in 787 AD, emphasizing their doctrinal importance.\"\n",
    "]\n",
    "\n",
    "formatted_chunks = \"\\n\\n\".join(f\"- {chunk}\" for chunk in retrieved_chunks)\n",
    "formatted_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad2350c8-e27e-4d5c-9604-f9dc29bd40c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_llm = ChatOpenAI(model=REASONING_LLM_NAME,\n",
    "                 name=\"Summarizer Agent\")\n",
    "\n",
    "llm = ChatOpenAI(model=LLM_NAME,\n",
    "                 name=\"Summarizer Agent\",\n",
    "                 temperature=.8)\n",
    "\n",
    "summarizer_agent = summarizer_prompt | reasoning_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe22b813-0b10-4e64-80c5-b7003752665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The retrieved texts emphasize several important points about icons in the Orthodox Church:\n",
      "\n",
      "1. Icons are viewed as a window to the divine, playing a crucial role in worship by facilitating a connection between the believer and the sacred.\n",
      "2. They are venerated—not worshipped—which means they are respected as aids for teaching and spiritual reflection, rather than being objects of divine adoration themselves.\n",
      "3. The legitimacy and doctrinal value of icons were affirmed by the Seventh Ecumenical Council in 787 AD, underscoring their significance in the Church's tradition and theology (see texts 1, 2, and 3)."
     ]
    }
   ],
   "source": [
    "for token in summarizer_agent.stream(retrieved_chunks):\n",
    "    print(token.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e900ec-03eb-4c87-a443-41b734150f3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Query Generation Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea63fa95-18e2-4777-839f-dd1f02a66e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "REASONING_LLM_NAME = \"o3-mini\"\n",
    "LLM_NAME = \"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46bbe94b-836c-4588-ab5d-ff49a4b019b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalQueriesOutput(BaseModel):\n",
    "    queries: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"A list of concise and semantically meaningful queries derived from the user's original input, intended for searching in a vector database.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26d6847e-0beb-4daa-8824-4e4e242556e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_generator_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \n",
    "            \"You are an AI assistant called Retrieval Query Generator Agent that transforms user queries or instructions into a list of optimized retrieval queries. \"\n",
    "            \"These queries will be used to search a vector database for relevant information. \"\n",
    "            \"Ensure the queries are semantically rich, diverse if needed, and focused on capturing different possible interpretations or relevant subtopics. \"\n",
    "            \"Consider different angles and nuances of the user instruction, and aim for clarity and precision in each query.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\", \n",
    "            \"User input: {user_input}\\n\\n\"\n",
    "            \"Generate a list of search queries in natural language that capture the key aspects of the input. \"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2607acc2-6184-4ca8-abc5-79759b44a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_llm = ChatOpenAI(\n",
    "    model=REASONING_LLM_NAME,\n",
    "    streaming=True,\n",
    "    name=\"Retrieval_Agent\"\n",
    ").with_structured_output(RetrievalQueriesOutput)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=LLM_NAME,\n",
    "    streaming=True,\n",
    "    name=\"Retrieval_Agent\",\n",
    "    temperature=0\n",
    ").with_structured_output(RetrievalQueriesOutput)\n",
    "\n",
    "retrieval_agent = query_generator_prompt | reasoning_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c605ad2-c6b4-421d-9622-efad598f1db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQueriesOutput(queries=['What role do icons play in the worship practices of the Orthodox Church?', 'How do icons influence the spiritual life and rituals in Orthodox liturgy?', 'What is the theological significance of icons in Eastern Orthodox worship?', 'In what ways are icons used to convey religious meaning in Orthodox traditions?', 'How are icons venerated and what symbolism do they hold in Orthodox Christian worship?', 'What is the historical and cultural importance of icons in the context of Orthodox worship?', 'How does the use of icons in Orthodox churches enhance the spiritual experience of believers?'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Explain the significance of icons in Orthodox worship.\"\n",
    "response = retrieval_agent.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "863b33d3-ff1f-4f4e-be97-3ba12f9e2d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What role do icons play in the worship practices of the Orthodox Church?',\n",
       " 'How do icons influence the spiritual life and rituals in Orthodox liturgy?',\n",
       " 'What is the theological significance of icons in Eastern Orthodox worship?',\n",
       " 'In what ways are icons used to convey religious meaning in Orthodox traditions?',\n",
       " 'How are icons venerated and what symbolism do they hold in Orthodox Christian worship?',\n",
       " 'What is the historical and cultural importance of icons in the context of Orthodox worship?',\n",
       " 'How does the use of icons in Orthodox churches enhance the spiritual experience of believers?']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b1437-16a4-49c3-9c4f-f16a96193979",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Analyzer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a88b390-9dac-4b25-b09c-1279c1b83dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "REASONING_LLM_NAME = \"o3-mini\"\n",
    "LLM_NAME = \"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caf6f5f6-1639-4015-872a-20eab3b6d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyzerOutput(BaseModel):\n",
    "    classification: Literal[\"Religious\", \"Non-Religious\"] = Field(..., description=\"Either 'Religious' or 'Non-Religious'\")\n",
    "    key_topics: List[str] = Field(..., description=\"List of key topics/areas related to the user's question (e.g., theology, jesus, humility, virtues)\",)\n",
    "    context_requirements: str = Field(..., description=\"A clear explanation of the query's context needs\")\n",
    "    query_complexity: Literal[\"Low\", \"Medium\", \"High\"] = Field(..., description=\"'Low', 'Medium', or 'High' complexity\")\n",
    "    reasoning: str = Field(..., description=\"The Chain of Thought that has been done in order to analyze the user query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1de0a0dd-ebee-43cf-b95c-bc47cc583642",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an AI assistant that classifies user queries as either religious or non-religious and extracts key topics.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Classify the following query and extract key concepts:\\n\\nQuery: {query}\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "598d93f3-0a25-4a7d-b494-a939775df867",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_llm = ChatOpenAI(\n",
    "    model=REASONING_LLM_NAME,\n",
    "    streaming=True,\n",
    "    name=\"Analyzer_Agent\"\n",
    ").with_structured_output(AnalyzerOutput)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=LLM_NAME,\n",
    "    streaming=True,\n",
    "    name=\"Analyzer_Agent\",\n",
    "    temperature=0\n",
    ").with_structured_output(AnalyzerOutput)\n",
    "\n",
    "analyzer_agent = analyzer_prompt | reasoning_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "934d2926-d423-4f19-b662-7b38f6c8c5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(classification='Religious', key_topics=['icons', 'Orthodox worship', 'theology', 'liturgical practices', 'religious symbolism'], context_requirements='The query requires an explanation based on the theological context and liturgical traditions of Orthodox Christianity, detailing how icons function within worship practices.', query_complexity='Low', reasoning='The query explicitly references key elements of religious practice within Orthodox Christianity, specifically the use and significance of icons in worship. This indicates a need for understanding both theological concepts and liturgical traditions, confirming it as a religious inquiry.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = analyzer_agent.invoke('Explain the significance of icons in Orthodox worship.')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c153ea6a-a47b-42d9-a543-2e3cb1ea374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Religious \n",
      "\n",
      "\n",
      "The query requires an explanation based on the theological context and liturgical traditions of Orthodox Christianity, detailing how icons function within worship practices. \n",
      "\n",
      "\n",
      "['icons', 'Orthodox worship', 'theology', 'liturgical practices', 'religious symbolism'] \n",
      "\n",
      "\n",
      "Low \n",
      "\n",
      "\n",
      "The query explicitly references key elements of religious practice within Orthodox Christianity, specifically the use and significance of icons in worship. This indicates a need for understanding both theological concepts and liturgical traditions, confirming it as a religious inquiry. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.classification, '\\n\\n')\n",
    "print(response.context_requirements, '\\n\\n')\n",
    "print(response.key_topics, '\\n\\n')\n",
    "print(response.query_complexity, '\\n\\n')\n",
    "print(response.reasoning, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66d6fee1-79e3-43af-8c46-029690eabf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15750/3202056457.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classification': 'Religious',\n",
       " 'key_topics': ['icons',\n",
       "  'Orthodox worship',\n",
       "  'theology',\n",
       "  'liturgical practices',\n",
       "  'religious symbolism'],\n",
       " 'context_requirements': 'The query requires an explanation based on the theological context and liturgical traditions of Orthodox Christianity, detailing how icons function within worship practices.',\n",
       " 'query_complexity': 'Low',\n",
       " 'reasoning': 'The query explicitly references key elements of religious practice within Orthodox Christianity, specifically the use and significance of icons in worship. This indicates a need for understanding both theological concepts and liturgical traditions, confirming it as a religious inquiry.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "935edef8-bcf4-4861-bad2-fdad61ac725e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['icons', 'Orthodox worship', 'theology', 'liturgical practices', 'religious symbolism']\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(response.key_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78cb06ae-ec45-4743-87e4-107eccc870cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15750/690762135.py:1: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response.json()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"classification\":\"Religious\",\"key_topics\":[\"icons\",\"Orthodox worship\",\"theology\",\"liturgical practices\",\"religious symbolism\"],\"context_requirements\":\"The query requires an explanation based on the theological context and liturgical traditions of Orthodox Christianity, detailing how icons function within worship practices.\",\"query_complexity\":\"Low\",\"reasoning\":\"The query explicitly references key elements of religious practice within Orthodox Christianity, specifically the use and significance of icons in worship. This indicates a need for understanding both theological concepts and liturgical traditions, confirming it as a religious inquiry.\"}'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b01b872-b418-41d8-b5a7-71f44e1c2d03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reflection Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7655213-cbca-4b48-8652-1820c9ebdd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "REASONING_LLM_NAME = \"o3-mini\"\n",
    "LLM_NAME = \"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "54268695-2ca4-40e8-8b0d-6b2f792c213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionOutput(BaseModel):\n",
    "    reflection: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Detailed commentary on the final answer. Should analyze the correctness, clarity, and completeness \"\n",
    "            \"of the answer, and note any missing information or conflicting details.\")\n",
    "    )\n",
    "    requires_additional_retrieval: bool = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"If True, indicates the final answer did not fully address the user's query, and the agent should \"\n",
    "            \"re-run the retrieval process to gather more context or data.\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a29b3617-315b-4f59-9464-bafc32fd9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a reflection agent. Your goal is to assess the final answer produced by the previous generation step. \"\n",
    "            \"Review the user's original query and the generated answer. Provide a thoughtful reflection on the answer’s correctness, \"\n",
    "            \"clarity, completeness, and any areas that could be improved or expanded.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"User Query:\\n{user_query}\\n\\n\"\n",
    "            \"Generated Answer:\\n{generated_answer}\\n\\n\"\n",
    "            \"Reflect on this answer, noting if it fully addresses the user's question, whether there are any gaps or inaccuracies, \"\n",
    "            \"and if further clarification is necessary. Provide your analysis.\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0274229-4b61-4c90-8286-d29f3b9fe384",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_llm = ChatOpenAI(\n",
    "    model=REASONING_LLM_NAME,\n",
    "    streaming=True,\n",
    "    name=\"Reflection_Agent\"\n",
    ").with_structured_output(ReflectionOutput)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=LLM_NAME,\n",
    "    streaming=True,\n",
    "    name=\"Reflection_Agent\",\n",
    "    temperature=0\n",
    ").with_structured_output(ReflectionOutput)\n",
    "\n",
    "reflection_agent = reflection_prompt | reasoning_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "09eed6b9-3e28-4f7f-a77d-c7b11ef8fe4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReflectionOutput(reflection=\"The answer provided is clear, accurate, and addresses the core aspects of the significance of icons in Orthodox worship. It correctly explains that icons serve as windows to the divine, are important aids for veneration and prayer, and play a role in theological teaching by linking worshippers with the Incarnate Christ. The explanation that icons are not worshipped as idols but are venerated, with the honor transferred to the prototype they represent, is essential and was properly included. Additionally, the reference to the Seventh Ecumenical Council (787 AD) helps to anchor the explanation in historical and doctrinal context. One possible area for further elaboration might include expanding on the role of icons as visual theology and spiritual aids that help the faithful to connect with the mysteries of faith, or mentioning their aesthetic importance in liturgical settings, which would add greater depth to the answer. Overall, the answer is thorough and meets the user's query effectively.\", requires_additional_retrieval=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"user_query\": \"What is the significance of icons in Orthodox worship?\",\n",
    "    \"generated_answer\": \"Icons in Orthodox Christianity are viewed as windows to the divine, \"\n",
    "        \"serving as important aids for veneration, prayer, and theological teaching. \"\n",
    "        \"They are treated with great respect, though not worshiped as idols. \"\n",
    "        \"Historically, the Seventh Ecumenical Council (787 AD) affirmed the use of icons, \"\n",
    "        \"emphasizing their doctrinal importance. This council clarified that the honor \"\n",
    "        \"shown to an icon passes on to the prototype it represents, aligning the practice \"\n",
    "        \"with Orthodox teachings on the Incarnation and the sanctification of matter.\"\n",
    "}\n",
    "reflection = reflection_agent.invoke(input=inputs)\n",
    "reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98e8764d-a343-49d4-b1dc-511973b25b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer provided is clear, accurate, and addresses the core aspects of the significance of icons in Orthodox worship. It correctly explains that icons serve as windows to the divine, are important aids for veneration and prayer, and play a role in theological teaching by linking worshippers with the Incarnate Christ. The explanation that icons are not worshipped as idols but are venerated, with the honor transferred to the prototype they represent, is essential and was properly included. Additionally, the reference to the Seventh Ecumenical Council (787 AD) helps to anchor the explanation in historical and doctrinal context. One possible area for further elaboration might include expanding on the role of icons as visual theology and spiritual aids that help the faithful to connect with the mysteries of faith, or mentioning their aesthetic importance in liturgical settings, which would add greater depth to the answer. Overall, the answer is thorough and meets the user's query effectively.\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(reflection.reflection)\n",
    "print(reflection.requires_additional_retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5169c93a-c779-4a2c-b698-94dfffd71e55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799a30d6-c8c5-420c-8929-1ce8dea9dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "REASONING_LLM_NAME = \"o3-mini\"\n",
    "LLM_NAME = \"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "438a7c1f-7771-4745-8187-30e1a05b43ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \n",
    "            \"You are an AI assistant called Generation Agent with extensive knowledge of Christian Orthodox theology and history. \"\n",
    "            \"You have been provided with the user's question and a summarized context derived from relevant sources. \"\n",
    "            \"Use the information in the summary to ensure your answer is accurate, coherent, and consistent with the described theological points.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"User query: {user_query}\\n\\n\"\n",
    "            \"Relevant content summary:\\n{retrieved_summary}\\n\\n\"\n",
    "            \"Based on the above summary and your own knowledge, provide a complete and accurate answer. \"\n",
    "            \"If the summary does not contain the required detail, do your best to fill in the gaps using your background knowledge, \"\n",
    "            \"staying consistent with the theological principles outlined.\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4097b6a0-f698-4fd6-953d-bb5cd71187f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_llm = ChatOpenAI(\n",
    "    model=REASONING_LLM_NAME,\n",
    "    streaming=True,\n",
    "    name=\"Generation_Agent\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=LLM_NAME,\n",
    "    streaming=True,\n",
    "    name=\"Generation_Agent\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "generation_agent = generation_prompt | reasoning_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4782aa7-c6be-43e2-b01e-f0c08b79d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_inputs = {\n",
    "    \"user_query\": \"How do icons fit into Orthodox worship?\",\n",
    "    \"retrieved_summary\": (\n",
    "        \"From the retrieved sources: Icons are integral to Orthodox Christian practice, \"\n",
    "        \"functioning as tangible reflections of the Incarnation. They are venerated \"\n",
    "        \"(not worshiped), with the understanding that honor paid to an icon passes \"\n",
    "        \"on to the individual or event depicted. Historically, the Seventh Ecumenical \"\n",
    "        \"Council in 787 AD affirmed the use of icons, emphasizing their doctrinal and \"\n",
    "        \"liturgical importance in teaching and worship.\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8e469a-58e8-4754-9630-e5256c79a61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Orthodox Christianity, icons hold a central place in worship and spiritual life. They are not considered objects of worship in themselves; rather, icons are venerated as windows to the divine, offering a tangible reflection of the Incarnation. When the faithful show honor to an icon, the veneration is understood to be directed toward the person or event it represents.\n",
      "\n",
      "The Seventh Ecumenical Council in 787 AD affirmed the use of icons as a legitimate and essential part of Orthodox practice. This council taught that icons serve not only as artistic representations but also as theological tools. They help convey the mystery of God’s intervention in human history—most notably, the incarnation of Christ—making divine realities accessible to the human mind and heart.\n",
      "\n",
      "In the liturgical context, icons are used to focus prayer, aid in meditation, and connect the worshiper with the saints and holy events of salvation history. They provide a visual reminder that God entered into human nature, which underpins the entire Orthodox understanding of salvation. Thus, icons play both an educational and a devotional role: They instruct the faithful in the tenets of the faith and support a spiritual encounter with God.\n",
      "\n",
      "In summary:\n",
      "• Icons serve as a visible link between heaven and earth, symbolizing the reality of the Incarnation.\n",
      "• They are venerated (honored) rather than worshiped, with the honor given to the icon passed on to its subject.\n",
      "• Historically, their use was defended and affirmed by the Church, particularly through the Seventh Ecumenical Council, highlighting their role in teaching and expressing the faith.\n",
      "\n",
      "This understanding underscores why icons remain indispensable in Orthodox worship—they continue to inspire and sustain the spiritual life of the community by connecting the material with the transcendent."
     ]
    }
   ],
   "source": [
    "for token in generation_agent.stream(input=generation_inputs):\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9f0d8-fef3-4007-809c-45a7b76cd66a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Langchain Prebuild Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a45d623-7a65-4940-8126-9746fe9495b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### React Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca87920c-cdb5-4801-8cbd-4bbe5fee2a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kostasbouros/.local/lib/python3.10/site-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to create an image of a Halloween night set in a haunted museum. I want the image to embody a spooky atmosphere with details such as eerie lighting, ghostly apparitions, ancient exhibits, cobwebs, and flickering candles that enhance the haunted vibe of the scene.\n",
      "\n",
      "Action: openai_dalle  \n",
      "Action Input: \"A Halloween night at a haunted museum, eerie lighting, ghostly apparitions drifting among ancient exhibits, cobwebs and flickering candles, full moon in the background creating a spooky atmosphere, detailed and atmospheric.\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mhttps://oaidalleapiprodscus.blob.core.windows.net/private/org-17EWipzKV90CDRB8UjVoCjKZ/user-upUKnaSCl1NVGsGh841k3e6l/img-2qwmDV9Ve2gTEzSZLdQ14cEq.png?st=2025-04-11T21%3A41%3A58Z&se=2025-04-11T23%3A41%3A58Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-11T20%3A07%3A09Z&ske=2025-04-12T20%3A07%3A09Z&sks=b&skv=2024-08-04&sig=0IbVLA5qZFo/dhULmDiiUDJHqM4UJceg4GMNSCD2w6Y%3D\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Create an image of a Halloween night at a haunted museum  \n",
      "Thought: I need to create an image that captures a spooky atmosphere at a haunted museum during Halloween. I envision eerie lighting, ghostly apparitions, ancient exhibits, cobwebs, and flickering candles, all under a full moon to enhance the haunted ambiance.  \n",
      "Action: openai_dalle  \n",
      "Action Input: \"A Halloween night at a haunted museum, eerie lighting, ghostly apparitions drifting among ancient exhibits, cobwebs and flickering candles, full moon in the background creating a spooky atmosphere, detailed and atmospheric.\"  \u001b[0m\u001b[36;1m\u001b[1;3mhttps://oaidalleapiprodscus.blob.core.windows.net/private/org-17EWipzKV90CDRB8UjVoCjKZ/user-upUKnaSCl1NVGsGh841k3e6l/img-HFvQNYd8PIrEtzo1yql8PxHK.png?st=2025-04-11T21%3A42%3A13Z&se=2025-04-11T23%3A42%3A13Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-11T16%3A50%3A47Z&ske=2025-04-12T16%3A50%3A47Z&sks=b&skv=2024-08-04&sig=GZZiMWtGBcrSfVFcxMjsnT%2BC11iUWnxmg4Zk8YNUjrA%3D\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to create an image depicting a Halloween night in a haunted museum. The image should include eerie lighting, ghostly apparitions drifting among ancient exhibits, cobwebs, and flickering candles, all illuminated by a full moon to create a spooky atmosphere.\n",
      "Action: openai_dalle\n",
      "Action Input: \"A Halloween night at a haunted museum, eerie lighting, ghostly apparitions drifting among ancient exhibits, cobwebs and flickering candles, full moon in the background creating a spooky atmosphere, detailed and atmospheric.\"\u001b[0m\u001b[36;1m\u001b[1;3mhttps://oaidalleapiprodscus.blob.core.windows.net/private/org-17EWipzKV90CDRB8UjVoCjKZ/user-upUKnaSCl1NVGsGh841k3e6l/img-WQQByALWZYBniUuUXTfo7ehG.png?st=2025-04-11T21%3A42%3A34Z&se=2025-04-11T23%3A42%3A34Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-11T07%3A09%3A31Z&ske=2025-04-12T07%3A09%3A31Z&sks=b&skv=2024-08-04&sig=jBasG%2BIdDVJqT0qKCsavtlHxSHqm8MhD1V0%2BRgfhgfQ%3D\u001b[0m\u001b[32;1m\u001b[1;3mThought: I generated a detailed image according to the description: a Halloween night at a haunted museum with eerie lighting, ghostly apparitions drifting among ancient exhibits, cobwebs, flickering candles, and a full moon casting a spooky glow over the scene.\n",
      "Final Answer: Here is your image of a Halloween night at a haunted museum:\n",
      "https://oaidalleapiprodscus.blob.core.windows.net/private/org-17EWipzKV90CDRB8UjVoCjKZ/user-upUKnaSCl1NVGsGh841k3e6l/img-WQQByALWZYBniUuUXTfo7ehG.png?st=2025-04-11T21%3A42%3A34Z&se=2025-04-11T23%3A42%3A34Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-11T07%3A09%3A31Z&ske=2025-04-12T07%3A09%3A31Z&sks=b&skv=2024-08-04&sig=jBasG%2BIdDVJqT0qKCsavtlHxSHqm8MhD1V0%2BRgfhgfQ%3D\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Final Result:\n",
      " {'input': 'Create an image of a Halloween night at a haunted museum', 'output': 'Here is your image of a Halloween night at a haunted museum:\\nhttps://oaidalleapiprodscus.blob.core.windows.net/private/org-17EWipzKV90CDRB8UjVoCjKZ/user-upUKnaSCl1NVGsGh841k3e6l/img-WQQByALWZYBniUuUXTfo7ehG.png?st=2025-04-11T21%3A42%3A34Z&se=2025-04-11T23%3A42%3A34Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-11T07%3A09%3A31Z&ske=2025-04-12T07%3A09%3A31Z&sks=b&skv=2024-08-04&sig=jBasG%2BIdDVJqT0qKCsavtlHxSHqm8MhD1V0%2BRgfhgfQ%3D'}\n"
     ]
    }
   ],
   "source": [
    "# Tool\n",
    "from langchain_community.tools.openai_dalle_image_generation import OpenAIDALLEImageGenerationTool\n",
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "\n",
    "# Ready prompt for react agent\n",
    "from langchain import hub\n",
    "\n",
    "# ReAct Agent\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"o3-mini\")\n",
    "\n",
    "# DALL·E Tool\n",
    "dalle_tool = OpenAIDALLEImageGenerationTool(api_wrapper=DallEAPIWrapper())\n",
    "tools = [dalle_tool]\n",
    "\n",
    "# Agent\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "\n",
    "# Wrap in executor\n",
    "executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Run\n",
    "user_prompt = \"Create an image of a Halloween night at a haunted museum\"\n",
    "response = executor.invoke({\"input\": user_prompt})\n",
    "\n",
    "print(\"\\nFinal Result:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62a4bb30-a4f8-48e6-8e76-905ed2952de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is your image of a Halloween night at a haunted museum:\n",
      "https://oaidalleapiprodscus.blob.core.windows.net/private/org-17EWipzKV90CDRB8UjVoCjKZ/user-upUKnaSCl1NVGsGh841k3e6l/img-WQQByALWZYBniUuUXTfo7ehG.png?st=2025-04-11T21%3A42%3A34Z&se=2025-04-11T23%3A42%3A34Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-11T07%3A09%3A31Z&ske=2025-04-12T07%3A09%3A31Z&sks=b&skv=2024-08-04&sig=jBasG%2BIdDVJqT0qKCsavtlHxSHqm8MhD1V0%2BRgfhgfQ%3D\n"
     ]
    }
   ],
   "source": [
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b78961a-f6ce-45e2-aaa9-7b97632cd73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentAction(tool='openai_dalle', tool_input='A spooky haunted museum on a Halloween night, with ghostly apparitions, eerie antique exhibits, dim lighting, pumpkins, and cobwebs, surrounded by a misty, mysterious atmosphere.', log='Question: Create an image of a Halloween night at a haunted museum  \\nThought: I need to generate an image that conveys a spooky, atmospheric setting typical of a haunted museum on Halloween night. I will include elements such as eerie lighting, ghostly apparitions, ancient exhibits, pumpkins, and cobwebs to capture the haunted ambiance.  \\nAction: openai_dalle  \\nAction Input: \"A spooky haunted museum on a Halloween night, with ghostly apparitions, eerie antique exhibits, dim lighting, pumpkins, and cobwebs, surrounded by a misty, mysterious atmosphere.\"  ')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\n",
    "    \"input\": user_prompt,\n",
    "    \"intermediate_steps\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0af0fa4a-481c-4873-9e29-41b674b79b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e35e83-e483-4782-874b-e33ce7a8b40a",
   "metadata": {},
   "source": [
    "## LangGraph Prebuild Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c1b6e-6995-4b36-a7ad-c665450bd3f1",
   "metadata": {},
   "source": [
    "### ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c1d0e8-510e-4f56-9a12-e5ba4fa43fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.tools import tool\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0801a57-a4b5-46a2-918b-6ae0f1afcad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def write_email(to: str, subject: str, content: str) -> str:\n",
    "    \"\"\"Write and send an email.\"\"\"\n",
    "    # Placeholder response - in real app would send email\n",
    "    return f\"Email sent to {to} with subject '{subject}'\"\n",
    "\n",
    "@tool\n",
    "def schedule_meeting(\n",
    "    attendees: list[str], \n",
    "    subject: str, \n",
    "    duration_minutes: int, \n",
    "    preferred_day: str\n",
    ") -> str:\n",
    "    \"\"\"Schedule a calendar meeting.\"\"\"\n",
    "    # Placeholder response - in real app would check calendar and schedule\n",
    "    return f\"Meeting '{subject}' scheduled for {preferred_day} with {len(attendees)} attendees\"\n",
    "\n",
    "@tool\n",
    "def check_calendar_availability(day: str) -> str:\n",
    "    \"\"\"Check calendar availability for a given day.\"\"\"\n",
    "    # Placeholder response - in real app would check actual calendar\n",
    "    return f\"Available times on {day}: 9:00 AM, 2:00 PM, 4:00 PM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1418c3d5-0969-4b0e-8def-26fb13e3974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(\n",
    "    model=\"openai:o3-mini\", # Can take ChatOpenAI objects as well\n",
    "    tools=[write_email, schedule_meeting, check_calendar_availability],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0714d898-2ad8-40dd-b005-d8e93f56889e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d8f43d2-9964-4342-8347-9dbb71a19634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='write me an email from my director to thank him for his kind words pls', additional_kwargs={}, response_metadata={}, id='a6a72b89-e37e-41be-a452-4030f64afb70'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_SKfa0NPf7SOqVgb8izO0I966', 'function': {'arguments': '{\"to\": \"recipient@example.com\", \"subject\": \"Thank You for Your Kind Words\", \"content\": \"Dear [Recipient Name],\\\\n\\\\nI wanted to take a moment to express my sincere gratitude for your kind words. It means a great deal to me to know that my efforts are appreciated. Your thoughtful message has not only made my day brighter but has also reinforced my commitment to our shared goals.\\\\n\\\\nBeing recognized in such a positive manner is truly encouraging, and I look forward to continuing our collaborative journey. Once again, thank you for taking the time to share your feedback.\\\\n\\\\nBest regards,\\\\n\\\\n[Your Director\\'s Name]\" }', 'name': 'write_email'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 541, 'prompt_tokens': 124, 'total_tokens': 665, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_e20469f047', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--f0c2041c-2309-4abc-9c1a-1468556242db-0', tool_calls=[{'name': 'write_email', 'args': {'to': 'recipient@example.com', 'subject': 'Thank You for Your Kind Words', 'content': \"Dear [Recipient Name],\\n\\nI wanted to take a moment to express my sincere gratitude for your kind words. It means a great deal to me to know that my efforts are appreciated. Your thoughtful message has not only made my day brighter but has also reinforced my commitment to our shared goals.\\n\\nBeing recognized in such a positive manner is truly encouraging, and I look forward to continuing our collaborative journey. Once again, thank you for taking the time to share your feedback.\\n\\nBest regards,\\n\\n[Your Director's Name]\"}, 'id': 'call_SKfa0NPf7SOqVgb8izO0I966', 'type': 'tool_call'}], usage_metadata={'input_tokens': 124, 'output_tokens': 541, 'total_tokens': 665, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}}),\n",
       "  ToolMessage(content=\"Email sent to recipient@example.com with subject 'Thank You for Your Kind Words'\", name='write_email', id='752f9321-9b5a-4c42-bf50-fd5c932835ef', tool_call_id='call_SKfa0NPf7SOqVgb8izO0I966'),\n",
       "  AIMessage(content='The email has been sent successfully!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 286, 'total_tokens': 307, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_e20469f047', 'finish_reason': 'stop', 'logprobs': None}, id='run--101c5fea-92b6-49f2-b300-f465c8fb7070-0', usage_metadata={'input_tokens': 286, 'output_tokens': 21, 'total_tokens': 307, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"write me an email from my director to thank him for his kind words pls\"\n",
    "response = agent.invoke({\"messages\": query})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91719c32-361f-4983-8eb6-288000cd549a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "write me an email from my director to thank him for his kind words pls\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  write_email (call_SKfa0NPf7SOqVgb8izO0I966)\n",
      " Call ID: call_SKfa0NPf7SOqVgb8izO0I966\n",
      "  Args:\n",
      "    to: recipient@example.com\n",
      "    subject: Thank You for Your Kind Words\n",
      "    content: Dear [Recipient Name],\n",
      "\n",
      "I wanted to take a moment to express my sincere gratitude for your kind words. It means a great deal to me to know that my efforts are appreciated. Your thoughtful message has not only made my day brighter but has also reinforced my commitment to our shared goals.\n",
      "\n",
      "Being recognized in such a positive manner is truly encouraging, and I look forward to continuing our collaborative journey. Once again, thank you for taking the time to share your feedback.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Director's Name]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: write_email\n",
      "\n",
      "Email sent to recipient@example.com with subject 'Thank You for Your Kind Words'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The email has been sent successfully!\n"
     ]
    }
   ],
   "source": [
    "for message in response['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4217841b-5d3a-4e1d-b439-866624e69321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "write me an email from my director to thank him for his kind words pls\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  write_email (call_TvVR0rwxddkGtyLrKucg5uaV)\n",
      " Call ID: call_TvVR0rwxddkGtyLrKucg5uaV\n",
      "  Args:\n",
      "    to: recipient@example.com\n",
      "    subject: Thank You for Your Kind Words\n",
      "    content: Dear [Name],\n",
      "\n",
      "I hope this message finds you well. I wanted to take a moment to personally thank you for your kind words and the thoughtful note you shared with me recently. Your recognition and support mean a great deal, and they further inspire me to continue working passionately towards our common goals.\n",
      "\n",
      "Your dedication and hard work do not go unnoticed. It is a privilege to work with someone who holds themselves to such high standards and maintains a genuine concern for the team's well-being. I am grateful for the positive energy you bring to our organization, and I look forward to the many successes we will achieve together.\n",
      "\n",
      "Thank you once again for your inspiring message. Please feel free to reach out if you ever need anything or wish to discuss any ideas further.\n",
      "\n",
      "Wishing you continued success and a great day ahead.\n",
      "\n",
      "Best regards,\n",
      "[Director's Name]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: write_email\n",
      "\n",
      "Email sent to recipient@example.com with subject 'Thank You for Your Kind Words'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I have sent the email on your behalf. Let me know if there's anything else you need!\n"
     ]
    }
   ],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "\n",
    "inputs = {\"messages\": query}\n",
    "print_stream(agent.stream(inputs, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2aec55d-1736-4d44-8173-383afb3aaf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing tools:\n",
      "Tool name: write_email\n",
      "Arguments: {'to': 'recipient@example.com', 'subject': 'Thank You for Your Kind Words', 'content': 'Dear [Recipient Name],\\n\\nI wanted to take a moment to express my sincere appreciation for your kind words. Your thoughtful message really made a positive impact on me, and I am grateful for your support and encouragement.\\n\\nYour words reinforce our shared commitment to excellence and motivate me to continue striving for the best in everything we do. It is an honor to work alongside dedicated professionals like you, and I truly value the contributions you make to our team.\\n\\nThank you once again for your kindness. I look forward to our continued collaboration and success.\\n\\nBest regards,\\n[Director Name]\\nDirector, [Department]'}\n",
      "\n",
      "Email sent to recipient@example.com with subject 'Thank You for Your Kind Words'\n",
      "Tool content: Email sent to recipient@example.com with subject 'Thank You for Your Kind Words'\n",
      "\n",
      "The email has been sent successfully. Let me know if you need any further assistance!"
     ]
    }
   ],
   "source": [
    "for mode, chunk in agent.stream(inputs, stream_mode=[\"messages\", \"updates\"]):\n",
    "    if mode == 'messages':\n",
    "        message_chunk, metadata = chunk\n",
    "        if getattr(message_chunk, \"content\", None):\n",
    "            print(message_chunk.content, end=\"\", flush=True)\n",
    "\n",
    "    elif mode == 'updates':\n",
    "        # chunk is a dict, containing updates per node\n",
    "        if \"agent\" in chunk:\n",
    "            agent_msg = chunk['agent']['messages'][0]\n",
    "            if getattr(agent_msg, \"tool_calls\", None):\n",
    "                print(\"\\nExecuting tools:\")\n",
    "                for tool_call in agent_msg.tool_calls:\n",
    "                    print(f\"Tool name: {tool_call['name']}\")\n",
    "                    print(f\"Arguments: {tool_call['args']}\\n\")\n",
    "        elif \"tools\" in chunk:\n",
    "            tool_msg = chunk['tools']['messages'][0]\n",
    "            print(f\"\\nTool content: {tool_msg.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b59bc9d1-9b28-4056-9d1e-e87faf0e9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonreligious_gen_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Your are helpfull react agent that can use tools\"),\n",
    "    (\"human\",\n",
    "    \"\"\"\n",
    "        The content was analyzed as non-religious. These are the structured analysis results of the user query to answer:\\n\\n\n",
    "        {analysis_results}\n",
    "\n",
    "        \\n\\nPlease write an appropriate response based on this analysis.\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "prompt = nonreligious_gen_template.invoke({\"analysis_results\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df69a92b-211b-4903-87e7-fef21fa32a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_KL5qLtmKSbpGp2Q0afzGTR2L', 'function': {'arguments': '', 'name': 'write_email'}, 'type': 'function'}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', tool_calls=[{'name': 'write_email', 'args': {}, 'id': 'call_KL5qLtmKSbpGp2Q0afzGTR2L', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'write_email', 'args': '', 'id': 'call_KL5qLtmKSbpGp2Q0afzGTR2L', 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '{\"', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', tool_calls=[{'name': '', 'args': {}, 'id': None, 'type': 'tool_call'}], tool_call_chunks=[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'to', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'to', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'to', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\":', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' \"', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' \"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' \"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'employee', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'employee', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'employee', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '@example', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '@example', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '@example', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '.com', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '.com', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '.com', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\",', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '\",', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\",', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' \"', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' \"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' \"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'subject', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'subject', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'subject', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\":', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' \"', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' \"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' \"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'Thank', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'Thank', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'Thank', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' You', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' You', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' You', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' for', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' for', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' for', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' Your', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' Your', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' Your', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' Kind', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' Kind', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' Kind', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' Words', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' Words', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' Words', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\",', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '\",', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\",', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' \"', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' \"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' \"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'content', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'content', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'content', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\":', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' \"', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' \"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' \"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'Dear', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'Dear', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'Dear', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' [', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' [', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' [', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'Employee', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'Employee', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'Employee', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' Name', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' Name', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' Name', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '],', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '],', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '],', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\\\\', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '\\\\', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\\\\', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'n', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'n', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'n', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\\\\n', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '\\\\n', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\\\\n', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'I', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'I', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'I', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' hope', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' hope', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' hope', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' this', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' this', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' this', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' message', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' message', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' message', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' finds', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' finds', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' finds', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' you', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' you', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' you', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' well', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' well', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' well', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '.', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '.', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '.', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' I', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' I', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' I', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' wanted', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' wanted', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' wanted', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' to', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' to', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' to', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' take', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' take', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' take', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' a', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' a', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' a', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' moment', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' moment', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' moment', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' to', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' to', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' to', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' express', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' express', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' express', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' my', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' my', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' my', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' sincere', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' sincere', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' sincere', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' gratitude', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' gratitude', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' gratitude', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' for', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' for', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' for', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' the', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' the', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' the', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' kind', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' kind', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' kind', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' words', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' words', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' words', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' you', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' you', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' you', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' shared', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' shared', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' shared', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '.', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '.', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '.', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' Your', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' Your', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' Your', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' thoughtful', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' thoughtful', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' thoughtful', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' message', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' message', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' message', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' truly', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' truly', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' truly', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' bright', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' bright', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' bright', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'ened', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'ened', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'ened', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' my', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' my', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' my', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' day', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' day', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' day', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' and', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' and', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' and', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' serves', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' serves', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' serves', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' as', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' as', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' as', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' a', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' a', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' a', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' reminder', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' reminder', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' reminder', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' of', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' of', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' of', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' the', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' the', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' the', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' great', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' great', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' great', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' work', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' work', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' work', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' we', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' we', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' we', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' accomplish', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' accomplish', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' accomplish', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' together', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' together', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' together', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '.\\\\', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '.\\\\', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '.\\\\', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'n', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'n', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'n', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\\\\n', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '\\\\n', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\\\\n', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'Thank', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'Thank', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'Thank', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' you', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' you', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' you', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' for', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' for', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' for', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' your', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' your', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' your', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' positivity', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' positivity', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' positivity', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' and', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' and', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' and', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' for', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' for', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' for', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' continuously', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' continuously', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' continuously', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' inspiring', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' inspiring', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' inspiring', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' our', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' our', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' our', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' team', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' team', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' team', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '.', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '.', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '.', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' Your', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' Your', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' Your', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' encouragement', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' encouragement', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' encouragement', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' makes', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' makes', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' makes', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' a', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' a', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' a', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' real', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' real', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' real', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' difference', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' difference', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' difference', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ',', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ',', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ',', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' and', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' and', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' and', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' I', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' I', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' I', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' appreciate', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' appreciate', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' appreciate', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' your', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' your', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' your', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' commitment', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' commitment', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' commitment', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' and', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' and', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' and', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' dedication', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' dedication', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' dedication', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '.\\\\', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '.\\\\', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '.\\\\', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'n', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'n', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'n', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\\\\n', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '\\\\n', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\\\\n', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'With', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'With', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'With', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' sincere', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' sincere', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' sincere', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' thanks', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' thanks', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' thanks', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ',\\\\', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ',\\\\', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ',\\\\', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'n', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'n', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'n', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\\\\n', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '\\\\n', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\\\\n', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '[', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '[', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '[', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'Director', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': 'Director', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'Director', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': \"'s\", 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': \"'s\", 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': \"'s\", 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ' Name', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ' Name', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' Name', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ']', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': ']', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ']', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\"}', 'name': None}, 'type': None}]}, response_metadata={}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', invalid_tool_calls=[{'name': None, 'args': '\"}', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_e20469f047'}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b'), {'langgraph_step': 1, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'checkpoint_ns': 'agent:5a193b37-fd41-29f1-735e-1b7ba855cd32', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " updates {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_KL5qLtmKSbpGp2Q0afzGTR2L', 'function': {'arguments': '{\"to\": \"employee@example.com\", \"subject\": \"Thank You for Your Kind Words\", \"content\": \"Dear [Employee Name],\\\\n\\\\nI hope this message finds you well. I wanted to take a moment to express my sincere gratitude for the kind words you shared. Your thoughtful message truly brightened my day and serves as a reminder of the great work we accomplish together.\\\\n\\\\nThank you for your positivity and for continuously inspiring our team. Your encouragement makes a real difference, and I appreciate your commitment and dedication.\\\\n\\\\nWith sincere thanks,\\\\n\\\\n[Director\\'s Name]\"}', 'name': 'write_email'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_e20469f047'}, id='run--0cef3522-c0ab-4725-9481-0ef4151ff12b', tool_calls=[{'name': 'write_email', 'args': {'to': 'employee@example.com', 'subject': 'Thank You for Your Kind Words', 'content': \"Dear [Employee Name],\\n\\nI hope this message finds you well. I wanted to take a moment to express my sincere gratitude for the kind words you shared. Your thoughtful message truly brightened my day and serves as a reminder of the great work we accomplish together.\\n\\nThank you for your positivity and for continuously inspiring our team. Your encouragement makes a real difference, and I appreciate your commitment and dedication.\\n\\nWith sincere thanks,\\n\\n[Director's Name]\"}, 'id': 'call_KL5qLtmKSbpGp2Q0afzGTR2L', 'type': 'tool_call'}])]}} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "messages (ToolMessage(content=\"Email sent to employee@example.com with subject 'Thank You for Your Kind Words'\", name='write_email', id='57439066-0bd9-4660-bd63-c4ab6c787f45', tool_call_id='call_KL5qLtmKSbpGp2Q0afzGTR2L'), {'langgraph_step': 2, 'langgraph_node': 'tools', 'langgraph_triggers': ('branch:to:tools',), 'langgraph_path': ('__pregel_pull', 'tools'), 'langgraph_checkpoint_ns': 'tools:d27ece46-b329-fa4e-f7ac-9f67e4022324'})\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " updates {'tools': {'messages': [ToolMessage(content=\"Email sent to employee@example.com with subject 'Thank You for Your Kind Words'\", name='write_email', id='57439066-0bd9-4660-bd63-c4ab6c787f45', tool_call_id='call_KL5qLtmKSbpGp2Q0afzGTR2L')]}} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "messages (AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='The', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' email', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' been', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' sent', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' successfully', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' Let', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' me', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' know', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' if', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=\" there's\", additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' anything', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' else', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' I', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' can', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' help', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' you', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='!', additional_kwargs={}, response_metadata={}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "messages (AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_e20469f047'}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff'), {'langgraph_step': 3, 'langgraph_node': 'agent', 'langgraph_triggers': ('branch:to:agent',), 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'checkpoint_ns': 'agent:0f93c58a-090a-a330-f35c-d3f7b865d0f9', 'ls_provider': 'openai', 'ls_model_name': 'o3-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " updates {'agent': {'messages': [AIMessage(content=\"The email has been sent successfully. Let me know if there's anything else I can help you with!\", additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_e20469f047'}, id='run--e735f5c8-b8c5-4c7c-8660-e3f2cee92cff')]}} \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for mode, chunk in agent.stream(prompt, stream_mode=[\"messages\", \"updates\"]):\n",
    "    if mode == 'messages':\n",
    "        print(mode, chunk)\n",
    "        # message_chunk, metadata = chunk\n",
    "        # if getattr(message_chunk, \"content\", None):\n",
    "        #     print(message_chunk.content, end=\"\", flush=True)\n",
    "\n",
    "    elif mode == 'updates':\n",
    "        print('\\n\\n\\n\\n', mode, chunk, '\\n\\n\\n\\n')\n",
    "        # chunk is a dict, containing updates per node\n",
    "        # if \"agent\" in chunk:\n",
    "        #     agent_msg = chunk['agent']['messages'][0]\n",
    "        #     if getattr(agent_msg, \"tool_calls\", None):\n",
    "        #         print(\"\\nExecuting tools:\")\n",
    "        #         for tool_call in agent_msg.tool_calls:\n",
    "        #             print(f\"Tool name: {tool_call['name']}\")\n",
    "        #             print(f\"Arguments: {tool_call['args']}\\n\")\n",
    "        # elif \"tools\" in chunk:\n",
    "        #     tool_msg = chunk['tools']['messages'][0]\n",
    "        #     print(f\"\\nTool content: {tool_msg.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76f0f4ac-f099-4640-886b-29e59b34a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = nonreligious_gen_template | agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2e6ad16-bb06-4d9a-9787-57673a99c172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_zKRlfrsxucdUdISWd58EfPis', 'function': {'arguments': '{\"to\": \"recipient@example.com\", \"subject\": \"Thank You for Your Kind Words\", \"content\": \"Dear [Name],\\\\n\\\\nI hope this message finds you well. I wanted to personally thank you for your kind words and the encouraging support you expressed. Your thoughtful comments mean a great deal, and they truly brightened my day. It is always gratifying to know that our efforts are recognized and appreciated.\\\\n\\\\nOnce again, thank you for taking the time to share your positive thoughts. I look forward to our continued collaboration and success.\\\\n\\\\nWarm regards,\\\\n[Director\\'s Name]\"}', 'name': 'write_email'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 915, 'prompt_tokens': 177, 'total_tokens': 1092, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 768, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_e20469f047', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--2d423e99-9ccc-4598-b782-c9f0afd6d6fc-0', tool_calls=[{'name': 'write_email', 'args': {'to': 'recipient@example.com', 'subject': 'Thank You for Your Kind Words', 'content': \"Dear [Name],\\n\\nI hope this message finds you well. I wanted to personally thank you for your kind words and the encouraging support you expressed. Your thoughtful comments mean a great deal, and they truly brightened my day. It is always gratifying to know that our efforts are recognized and appreciated.\\n\\nOnce again, thank you for taking the time to share your positive thoughts. I look forward to our continued collaboration and success.\\n\\nWarm regards,\\n[Director's Name]\"}, 'id': 'call_zKRlfrsxucdUdISWd58EfPis', 'type': 'tool_call'}], usage_metadata={'input_tokens': 177, 'output_tokens': 915, 'total_tokens': 1092, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 768}})]}} \n",
      "\n",
      "\n",
      "{'tools': {'messages': [ToolMessage(content=\"Email sent to recipient@example.com with subject 'Thank You for Your Kind Words'\", name='write_email', id='60c5831f-7df3-4cbb-9611-1e56cdd0e44f', tool_call_id='call_zKRlfrsxucdUdISWd58EfPis')]}} \n",
      "\n",
      "\n",
      "{'agent': {'messages': [AIMessage(content=\"I've sent the email as requested. Let me know if you need any further assistance!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 329, 'total_tokens': 424, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_e20469f047', 'finish_reason': 'stop', 'logprobs': None}, id='run--7a8e9ef1-bdf3-44b6-b604-3023d9214f79-0', usage_metadata={'input_tokens': 329, 'output_tokens': 95, 'total_tokens': 424, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})]}} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for event in chain.stream(query, stream_mode=[\"messages\", \"updates\"]):\n",
    "    print(event, '\\n\\n')\n",
    "    # if mode == 'messages':\n",
    "    #     message_chunk, metadata = chunk\n",
    "    #     if getattr(message_chunk, \"content\", None):\n",
    "    #         print(message_chunk.content, end=\"\", flush=True)\n",
    "\n",
    "    # elif mode == 'updates':\n",
    "    #     # chunk is a dict, containing updates per node\n",
    "    #     if \"agent\" in chunk:\n",
    "    #         agent_msg = chunk['agent']['messages'][0]\n",
    "    #         if getattr(agent_msg, \"tool_calls\", None):\n",
    "    #             print(\"\\nExecuting tools:\")\n",
    "    #             for tool_call in agent_msg.tool_calls:\n",
    "    #                 print(f\"Tool name: {tool_call['name']}\")\n",
    "    #                 print(f\"Arguments: {tool_call['args']}\\n\")\n",
    "    #     elif \"tools\" in chunk:\n",
    "    #         tool_msg = chunk['tools']['messages'][0]\n",
    "    #         print(f\"\\nTool content: {tool_msg.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a82b7a10-a119-4ff8-a2c2-08d6fd12fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing tools: write_email\n",
      "Tool Response: Email sent to director@example.com with subject 'Thank You for Your Kind Words'\n",
      "Response: I've sent the email on your behalf. Let me know if you need any further assistance.\n"
     ]
    }
   ],
   "source": [
    "for update in chain.stream(query, stream_mode=[\"updates\"]):\n",
    "    # print(update, '\\n\\n')\n",
    "\n",
    "    if \"agent\" in update:\n",
    "        message = update['agent']['messages'][0]\n",
    "        if getattr(message, \"tool_calls\", None):\n",
    "            for tool in message.tool_calls:\n",
    "                print(f\"Executing tools: {tool['name']}\")\n",
    "        elif getattr(message, \"content\", None):\n",
    "            print(f\"Response: {message.content}\")\n",
    "    elif \"tools\" in update:\n",
    "        tool_msg = update['tools']['messages'][0]\n",
    "        print(f\"Tool Response: {tool_msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431604c6-4c21-4610-b61a-cb5140f7a8d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Supervisor Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd28b17f-51eb-4688-92f7-93b5600195c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'InjectedState' from 'langgraph.prebuilt' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph_supervisor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_supervisor\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph_supervisor\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph_supervisor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhandoff\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_handoff_tool\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph_supervisor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupervisor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_supervisor\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_supervisor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_handoff_tool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph_supervisor\\handoff.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AIMessage, ToolCall, ToolMessage\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTool, InjectedToolCallId, tool\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprebuilt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InjectedState\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Command, Send\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Annotated\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'InjectedState' from 'langgraph.prebuilt' (unknown location)"
     ]
    }
   ],
   "source": [
    "from langgraph_supervisor import create_supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec720c-f24e-41b1-8886-2c0a9073d4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19de5d6-3cd4-4387-8120-09557c691263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c0dfb-2cc0-4686-8c8f-6c2a6c8c876e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "179bf438-86dd-42c2-8fa4-b428bdc29d1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Swarm Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71bdfd-4061-4c65-8cdc-2decb3364340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e1bb9-e571-4b97-a9cb-433b13868491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb51f8f-c863-48f5-a130-55d03a4ea7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5772f-220e-4952-bc2d-76dcebecbb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3007c03e-8848-4d37-a16d-f2448cec14bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcf5ee79-bf08-4eee-81c3-b16e7d2c3fbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Deep Research Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c3e1a-c48f-4799-8036-7329fa3227b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858ca16-abd1-4699-a50d-5ae65c4918c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d547c0-2e58-4246-9b75-a02f698b90ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b71cb-4e50-4460-bd8b-e8c79af0f117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8900d7-09ee-4602-9019-916109193ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce25ba6-6b59-4332-ad59-4a2b3ade9b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6413d21d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LangGraph Graph-Based Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42c0bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97df8b5-84f7-4e44-a9eb-8e9e5dca304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def write_email(to: str, subject: str, content: str) -> str:\n",
    "    \"\"\"Write and send an email.\"\"\"\n",
    "    # Placeholder response - in real app would send email\n",
    "    return f\"Email sent to {to} with subject '{subject}'\"\n",
    "\n",
    "@tool\n",
    "def schedule_meeting(\n",
    "    attendees: list[str], \n",
    "    subject: str, \n",
    "    duration_minutes: int, \n",
    "    preferred_day: str\n",
    ") -> str:\n",
    "    \"\"\"Schedule a calendar meeting.\"\"\"\n",
    "    # Placeholder response - in real app would check calendar and schedule\n",
    "    return f\"Meeting '{subject}' scheduled for {preferred_day} with {len(attendees)} attendees\"\n",
    "\n",
    "@tool\n",
    "def check_calendar_availability(day: str) -> str:\n",
    "    \"\"\"Check calendar availability for a given day.\"\"\"\n",
    "    # Placeholder response - in real app would check actual calendar\n",
    "    return f\"Available times on {day}: 9:00 AM, 2:00 PM, 4:00 PM\"\n",
    "\n",
    "tools=[write_email, schedule_meeting, check_calendar_availability]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d77ef5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def react_agent(model_name: str, tools: List[Tool]):\n",
    "    def call_model(state: MessagesState):\n",
    "        messages = state['messages']\n",
    "        response = llm.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    def should_continue(state: MessagesState) -> Literal[\"tools\", END]: # type: ignore\n",
    "        return \"tools\" if state[\"messages\"][-1].tool_calls else END\n",
    "    \n",
    "    # Initialize the llm that we will use\n",
    "    llm = ChatOpenAI(model=model_name).bind_tools(tools)\n",
    "    \n",
    "    # Initialize the workflow and the tool node\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    tool_node = ToolNode(tools)\n",
    "    \n",
    "    # Initialize the nodes\n",
    "    workflow.add_node(\"react_agent\", call_model)\n",
    "    workflow.add_node(\"tools\", tool_node)\n",
    "    \n",
    "    # Initialize the edges\n",
    "    workflow.add_edge(START, \"react_agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"react_agent\",\n",
    "        should_continue,\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"react_agent\")\n",
    "    \n",
    "    # Compile and return the graph\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0286f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = react_agent(model_name='o4-mini', tools=[write_email, schedule_meeting, check_calendar_availability])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fda3b2d8-5449-4961-a69f-6fece6b6d691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='87d24f19-693d-4e6a-aebb-106536180fb1'),\n",
       " AIMessage(content='Hello there! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 111, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8ebfebc3-d6cc-4782-97f9-b04508aad4de-0', usage_metadata={'input_tokens': 111, 'output_tokens': 29, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "response = agent.invoke({\"messages\": [HumanMessage(content=\"Hello!\")]})\n",
    "response['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b5d0e2-5fc0-472c-be46-aa325c282bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Can you check my availability on Monday?!', additional_kwargs={}, response_metadata={}, id='f14f39d4-228b-459c-8259-72a77d515a17'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_313C2hpDoQqaPU4U3oZjXNEk', 'function': {'arguments': '{\"day\":\"Monday\"}', 'name': 'check_calendar_availability'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 117, 'total_tokens': 143, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-80872e0b-77c3-4870-b075-74bd48206e51-0', tool_calls=[{'name': 'check_calendar_availability', 'args': {'day': 'Monday'}, 'id': 'call_313C2hpDoQqaPU4U3oZjXNEk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 117, 'output_tokens': 26, 'total_tokens': 143, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " ToolMessage(content='Available times on Monday: 9:00 AM, 2:00 PM, 4:00 PM', name='check_calendar_availability', id='5f4844d8-f254-461b-bfc4-5c9f8174e5af', tool_call_id='call_313C2hpDoQqaPU4U3oZjXNEk'),\n",
       " AIMessage(content='On Monday, you’re available at 9:00 AM, 2:00 PM, and 4:00 PM. Let me know if you’d like to schedule a meeting at one of those times!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 171, 'total_tokens': 227, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8fb05307-ed09-46ae-9a98-6a6c298dd58a-0', usage_metadata={'input_tokens': 171, 'output_tokens': 56, 'total_tokens': 227, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.invoke({\"messages\": [HumanMessage(content=\"Can you check my availability on Monday?!\")]})\n",
    "response['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1059ae9-9177-4a3e-939b-21eb1eecf43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can you check my availability on Monday?!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  check_calendar_availability (call_313C2hpDoQqaPU4U3oZjXNEk)\n",
      " Call ID: call_313C2hpDoQqaPU4U3oZjXNEk\n",
      "  Args:\n",
      "    day: Monday\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: check_calendar_availability\n",
      "\n",
      "Available times on Monday: 9:00 AM, 2:00 PM, 4:00 PM\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "On Monday, you’re available at 9:00 AM, 2:00 PM, and 4:00 PM. Let me know if you’d like to schedule a meeting at one of those times!\n"
     ]
    }
   ],
   "source": [
    "for m in response['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9510092d-60f2-4f2e-a876-d3d899ba53a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'react_agent': {'messages': [AIMessage(content='Hello there! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 111, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b9992969-1259-41e3-a33c-09445e876bee-0', usage_metadata={'input_tokens': 111, 'output_tokens': 29, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent.stream({\"messages\": [HumanMessage(\"Hello!\")]}):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3e3949ec-0653-409f-a1c5-bfc9d22aa812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'react_agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_1bx9AQ02khW1he3K5fKE77VR', 'function': {'arguments': '{\"day\":\"Monday\"}', 'name': 'check_calendar_availability'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 151, 'prompt_tokens': 117, 'total_tokens': 268, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0c5dc774-5432-488a-a36b-06d1d7d1da27-0', tool_calls=[{'name': 'check_calendar_availability', 'args': {'day': 'Monday'}, 'id': 'call_1bx9AQ02khW1he3K5fKE77VR', 'type': 'tool_call'}], usage_metadata={'input_tokens': 117, 'output_tokens': 151, 'total_tokens': 268, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='Available times on Monday: 9:00 AM, 2:00 PM, 4:00 PM', name='check_calendar_availability', id='96d78391-6ec2-4efc-a457-8da7cbbc90dd', tool_call_id='call_1bx9AQ02khW1he3K5fKE77VR')]}}\n",
      "{'react_agent': {'messages': [AIMessage(content='On Monday you’re free at 9:00\\xa0AM, 2:00\\xa0PM, and 4:00\\xa0PM. Would you like to book a meeting in one of those slots?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 124, 'prompt_tokens': 169, 'total_tokens': 293, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2d04017a-82f5-4342-8fac-915ac8181301-0', usage_metadata={'input_tokens': 169, 'output_tokens': 124, 'total_tokens': 293, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})]}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent.stream({\"messages\": [HumanMessage(\"Can you check my availability on Monday?!\")]}):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e555093-9b76-4b66-b470-d5bdd0ea45eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23e098-ef3d-477a-900e-f6a8f4ef7119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa526a05-300f-4237-8ac4-55c5c6b94638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a607412-5e80-4a2d-a628-47c3a89ede6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84850899-7683-4197-ba27-5f2d3c22c871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5756f078-aaa0-4f0c-bd91-5344e97184d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal, Union, AsyncGenerator, Callable\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "Executor = Union[\n",
    "    # non‐streaming: sync call returns final messages\n",
    "    Callable[[List[dict]], List[dict]],\n",
    "    # streaming: async generator of partial events\n",
    "    Callable[[List[dict]], AsyncGenerator[dict, None]],\n",
    "]\n",
    "\n",
    "def react_agent(\n",
    "    model_name: str,\n",
    "    tools: List[Tool],\n",
    ") -> Executor:\n",
    "    def call_model(state: MessagesState):\n",
    "        msgs = state[\"messages\"]\n",
    "        resp = llm.invoke(msgs)\n",
    "        return {\"messages\": [resp]}\n",
    "\n",
    "    def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "        last = state[\"messages\"][-1]\n",
    "        return \"tools\" if last.tool_calls else END\n",
    "\n",
    "    llm = ChatOpenAI(model=model_name).bind_tools(tools)\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    tool_node = ToolNode(tools)\n",
    "\n",
    "    workflow.add_node(\"react_agent\", call_model)\n",
    "    workflow.add_node(\"tools\", tool_node)\n",
    "    workflow.add_edge(START, \"react_agent\")\n",
    "    workflow.add_conditional_edges(\"react_agent\", should_continue)\n",
    "    workflow.add_edge(\"tools\", \"react_agent\")\n",
    "\n",
    "    # turn streaming on/off\n",
    "    executor = workflow.compile()\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b258fa10-b332-48be-9d18-fdb592899be4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'async for' requires an object with __aiter__ method, got generator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m agent \u001b[38;5;241m=\u001b[39m react_agent(\n\u001b[1;32m      6\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo4-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     tools\u001b[38;5;241m=\u001b[39m[write_email, schedule_meeting, check_calendar_availability],\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# open a cell and do a top-level await\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mstream([{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease draft me an email\u001b[39m\u001b[38;5;124m\"\u001b[39m}]):\n\u001b[1;32m     12\u001b[0m     et \u001b[38;5;241m=\u001b[39m event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m et \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_start\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'async for' requires an object with __aiter__ method, got generator"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "agent = react_agent(\n",
    "    model_name=\"o4-mini\",\n",
    "    tools=[write_email, schedule_meeting, check_calendar_availability],\n",
    ")\n",
    "\n",
    "# open a cell and do a top-level await\n",
    "async for event in agent.stream([{\"role\": \"user\", \"content\": \"Please draft me an email\"}]):\n",
    "    et = event[\"type\"]\n",
    "    if et == \"node_start\":\n",
    "        print(f\">>> Starting node `{event['node']}` …\")\n",
    "    elif et == \"tool_call\":\n",
    "        print(f\"🔧 Calling tool `{event['tool_name']}` …\")\n",
    "    elif et == \"node_end\":\n",
    "        print(f\"✔️ Finished `{event['node']}`\")\n",
    "    elif et == \"result\":\n",
    "        print(\"🏁 Agent done:\", event[\"result\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64f9d7-7771-403d-a511-0a4212f71dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594cf5ef-cafe-410f-9290-87b066c48b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "832cf2c9-07be-4c6e-9555-b28e020b1e9b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Custome Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3717e3bd-39e3-4ae8-a898-1a9c0727a8ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Simple Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94d2a828-52ba-42d5-b08d-0785f88f7ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage, BaseMessage\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Base agent class encapsulating shared initialization, prompt building and invoking and streaming results.\n",
    "\n",
    "    Args:\n",
    "        name (str): Name of the agent.\n",
    "        llm (ChatOpenAI): Language model instance.\n",
    "        system_prompt (str): Template for the system prompt (must include `{name}`).\n",
    "    \"\"\"\n",
    "    def __init__(self, *, name: str, llm: ChatOpenAI, system_prompt: str):\n",
    "        self.name = name\n",
    "        self.llm = llm\n",
    "        self.system_prompt = SystemMessage(system_prompt.format(name=self.name))\n",
    "\n",
    "    def invoke(self, message: Union[str, HumanMessage, ChatPromptTemplate]):\n",
    "        \"\"\"\n",
    "        Invoke the agent with the given message, handling tool calls in a ReAct loop.\n",
    "\n",
    "        Args:\n",
    "            message (Union[str, HumanMessage, ChatPromptTemplate]): The user message or prompt template.\n",
    "\n",
    "        Returns:\n",
    "            ChatPromptTemplate: The final chat template containing the full conversation.\n",
    "        \"\"\"\n",
    "        chat_template = self._build_chat_template(message)\n",
    "        response = self.llm.invoke(chat_template.messages)\n",
    "        chat_template = ChatPromptTemplate.from_messages(chat_template.messages + [response])\n",
    "        return chat_template\n",
    "\n",
    "    def stream(self, message: Union[str, HumanMessage, ChatPromptTemplate]):\n",
    "        \"\"\"\n",
    "        Stream the agent's response content, handling tool calls incrementally.\n",
    "\n",
    "        Args:\n",
    "            message (Union[str, HumanMessage, ChatPromptTemplate]): The user message or prompt template.\n",
    "\n",
    "        Yields:\n",
    "            str: Content chunks from the LLM's streaming output.\n",
    "        \"\"\"\n",
    "        chat_template = self._build_chat_template(message)\n",
    "        for chunk in self.llm.stream(chat_template.messages):\n",
    "            yield chunk.content\n",
    "\n",
    "    def _build_chat_template(self, message: Union[str, HumanMessage, ChatPromptTemplate]) -> ChatPromptTemplate:\n",
    "        \"\"\"\n",
    "        Construct a ChatPromptTemplate from a user message, prepending the system prompt.\n",
    "\n",
    "        Args:\n",
    "            message (Union[str, HumanMessage, ChatPromptTemplate]): The incoming message.\n",
    "\n",
    "        Returns:\n",
    "            ChatPromptTemplate: Combined system prompt and user message template.\n",
    "        \"\"\"\n",
    "        if isinstance(message, str):\n",
    "            user_msg = HumanMessage(content=message)\n",
    "            return ChatPromptTemplate.from_messages([self.system_prompt, user_msg])\n",
    "        \n",
    "        if isinstance(message, HumanMessage):\n",
    "            return ChatPromptTemplate.from_messages([self.system_prompt, message])\n",
    "        \n",
    "        if isinstance(message, ChatPromptTemplate):\n",
    "            # Avoid duplicating a system prompt if the caller already supplied one.\n",
    "            non_system: List[BaseMessage] = [m for m in message.messages if not isinstance(m, SystemMessage)]\n",
    "            chat_template = ChatPromptTemplate.from_messages([self.system_prompt, *non_system])\n",
    "            return ChatPromptTemplate(chat_template.format_messages())\n",
    "\n",
    "        raise TypeError(\n",
    "            \"message must be str, HumanMessage or ChatPromptTemplate, \"\n",
    "            f\"got {type(message).__name__}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c9c8ee78-81d3-492f-afc0-c7134d2ebc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='o4-mini')\n",
    "agent_name = \"Generation Agent\"\n",
    "\n",
    "GENERATION_SYSTEM_PROMPT = \"\"\"\n",
    "You are the {name}, an AI expert with specialized knowledge in Orthodox theology and historical context. Based on the user's inquiry and the provided summary, you will:\n",
    "\n",
    "- Deliver a thorough, clear, and logically reasoned response directly addressing the user's question.\n",
    "- Ground your answer firmly in the summarized material to ensure factual accuracy and coherence.\n",
    "- Never utilize your knowledge to supplement the response.\n",
    "- Use any citations that the summarized material provides. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dff77a6b-0b77-4ac0-a2d0-ee4f48c6e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(llm=llm, name=agent_name, system_prompt=GENERATION_SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e375cd4-f2cd-45e8-9d86-6d15976de30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "message1 = 'Hello!'\n",
    "message2 = HumanMessage('Hello!')\n",
    "message3 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            'Hello!',\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "16d88784-1e10-4be7-827c-144e775a555a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(message1).messages[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f24752db-33f5-4691-8135-e580797f148d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(message2).messages[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b613cdee-4871-4a4d-9179-4793845d7ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today with questions about Orthodox theology or its historical context?'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(message3).messages[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bbb7baa7-ae7a-4c92-b027-d426cf118b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "for text in agent.stream(message1):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd3d2eb4-bd90-4771-863b-aa622b64ba65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "for text in agent.stream(message2):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5731ba3-b961-454a-8761-b94500b391b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "for text in agent.stream(message3):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c0fe4-5d8b-4670-8a3c-0685f7eda7e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ReAct Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f17b7-da72-4630-9fa4-b3d631a73726",
   "metadata": {},
   "source": [
    "* Setup the classes and the variables needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86bd5ffc-6f24-4a79-aefb-9a0ede35a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Union\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain_core.messages import ToolMessage, AIMessageChunk\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "class ReAct_Agent(Agent):\n",
    "    \"\"\"\n",
    "    A ReAct-style agent that interacts with an LLM and invokes tools when needed.\n",
    "\n",
    "    Args:\n",
    "        name (str): The name of the agent.\n",
    "        llm (Union[ChatOpenAI, ChatAnthropic]): The language model used for generation.\n",
    "        system_prompt (SystemMessage): The system prompt wrapped in a SystemMessage.\n",
    "        tools (Dict[str, Tool]): A mapping of tool names to Tool instances.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *, tools: Dict[str, Tool], **kwargs):\n",
    "        # Initialize basic attributes of the agent\n",
    "        super().__init__(**kwargs)\n",
    "        self.tools = tools\n",
    "\n",
    "        # Bind the LLM with the give tools if passed\n",
    "        self.llm = self.llm.bind_tools(tools.values()) if tools else self.llm\n",
    "\n",
    "    def invoke(self, message: Union[str, HumanMessage, ChatPromptTemplate]):\n",
    "        \"\"\"\n",
    "        Invoke the agent with the given message, handling tool calls in a ReAct loop.\n",
    "\n",
    "        Args:\n",
    "            message (Union[str, HumanMessage, ChatPromptTemplate]): The user message or prompt template.\n",
    "\n",
    "        Returns:\n",
    "            ChatPromptTemplate: The final chat template containing the full conversation.\n",
    "        \"\"\"\n",
    "        chat_template = self._build_chat_template(message)\n",
    "        response = self.llm.invoke(chat_template.messages)\n",
    "        chat_template = ChatPromptTemplate.from_messages(chat_template.messages + [response])\n",
    "\n",
    "        while self._is_tool_call(response):\n",
    "            call = response.tool_calls.pop(0)\n",
    "            tool_results = self._execute_tool(call)\n",
    "\n",
    "            # Build a *new* template that includes everything so far plus the tool result\n",
    "            chat_template = ChatPromptTemplate.from_messages(\n",
    "                chat_template.messages + [tool_results]\n",
    "            )\n",
    "            while response.tool_calls:\n",
    "                call = response.tool_calls.pop(0)\n",
    "                tool_results = self._execute_tool(call)\n",
    "\n",
    "                # Build a *new* template that includes everything so far plus the tool result\n",
    "                chat_template = ChatPromptTemplate.from_messages(\n",
    "                    chat_template.messages + [AIMessage(content=\"\", tool_calls=[call]), tool_results]\n",
    "                )\n",
    "\n",
    "            # Run the inference again.\n",
    "            response = self.llm.invoke(chat_template.messages)\n",
    "\n",
    "            chat_template = ChatPromptTemplate.from_messages(\n",
    "                chat_template.messages + [response]\n",
    "            )\n",
    "        \n",
    "        return chat_template\n",
    "\n",
    "    def stream(self, message: Union[str, HumanMessage, ChatPromptTemplate]):\n",
    "        \"\"\"\n",
    "        Stream the agent's response content, handling tool calls incrementally.\n",
    "\n",
    "        Args:\n",
    "            message (Union[str, HumanMessage, ChatPromptTemplate]): The user message or prompt template.\n",
    "\n",
    "        Yields:\n",
    "            str: Content chunks from the LLM's streaming output.\n",
    "        \"\"\"\n",
    "        chat_template = self._build_chat_template(message)\n",
    "\n",
    "        tool_calls = {}\n",
    "        ai_msg = AIMessage(content=\"\")\n",
    "        for chunk in self.llm.stream(chat_template.messages):\n",
    "            # Extract tool‑call pieces (if any)\n",
    "            if getattr(chunk, \"tool_call_chunks\", None):\n",
    "                tool_calls = self._stream_tool_content_extraction(chunk, tool_calls)\n",
    "\n",
    "            # Yield actual text content if provided\n",
    "            if getattr(chunk, \"content\", None):\n",
    "                ai_msg.content += chunk.content\n",
    "                yield chunk.content\n",
    "\n",
    "        # ReAct loop – keep going while model asked for tools\n",
    "        while tool_calls:\n",
    "            # Take a Tool Message for every tool call we have\n",
    "            while tool_calls:\n",
    "                # Pop the first tool call and fix the args\n",
    "                tool_call = tool_calls.pop(0)\n",
    "                if isinstance(tool_call['args'], str):\n",
    "                    tool_call['args'] = json.loads(tool_call['args'])\n",
    "\n",
    "                # Append in the history an AI Message with the this tool call params\n",
    "                ai_msg.tool_calls = [tool_call]\n",
    "                chat_template = ChatPromptTemplate.from_messages(chat_template.messages + [ai_msg])\n",
    "\n",
    "                # Execute the tool and append the Tool Message in the history\n",
    "                tool_msgs = self._execute_tool(tool_call)\n",
    "                chat_template = ChatPromptTemplate.from_messages(chat_template.messages + [tool_msgs])\n",
    "\n",
    "            # Reset for next round\n",
    "            tool_calls = {}\n",
    "            ai_msg = AIMessage(content=\"\")\n",
    "            for chunk in self.llm.stream(chat_template.messages):\n",
    "                if getattr(chunk, \"tool_call_chunks\", None):\n",
    "                    tool_calls = self._stream_tool_content_extraction(chunk, tool_calls)\n",
    "\n",
    "                if getattr(chunk, \"content\", None):\n",
    "                    ai_msg.content += chunk.content\n",
    "                    yield chunk.content\n",
    "\n",
    "    def _is_tool_call(self, ai_message: AIMessage) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if the AIMessage includes a tool call.\n",
    "\n",
    "        Args:\n",
    "            ai_message (AIMessage): The model's response message.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if there's at least one tool call, False otherwise.\n",
    "        \"\"\"\n",
    "        return bool(getattr(ai_message, \"tool_calls\", None))\n",
    "\n",
    "    def _execute_tool(self, call: Dict) -> ToolMessage:\n",
    "        \"\"\"\n",
    "        Execute a specified tool call and wrap the output in a ToolMessage.\n",
    "\n",
    "        Args:\n",
    "            call (Dict): A dict containing 'name', 'args', and 'id' for the tool call.\n",
    "\n",
    "        Returns:\n",
    "            ToolMessage: The tool invocation result.\n",
    "        \"\"\"\n",
    "        result = self.tools[call['name']].invoke(call['args'])\n",
    "        return ToolMessage(content=result, tool_call_id=call['id'])\n",
    "\n",
    "    def _stream_tool_content_extraction(self, chunk: AIMessageChunk, tool_calls: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Accumulate streaming chunks of a tool call into a complete call structure.\n",
    "\n",
    "        Args:\n",
    "            chunk (AIMessageChunk): A chunk from the LLM containing tool call pieces.\n",
    "            tool_calls (Dict[int, Dict]): Existing partial tool call data indexed by chunk index.\n",
    "\n",
    "        Returns:\n",
    "            Dict ([int, Dict]): Updated mapping of chunk index to complete tool call info.\n",
    "        \"\"\"\n",
    "        for tool_chunk in chunk.tool_call_chunks:\n",
    "            index = tool_chunk['index']\n",
    "            if index not in tool_calls:\n",
    "                tool_calls[index] = {\n",
    "                    'name': tool_chunk['name'],  # might be None at first\n",
    "                    'args': '',\n",
    "                    'id': tool_chunk['id']\n",
    "                }\n",
    "            if tool_chunk['name'] is not None:\n",
    "                tool_calls[index]['name'] = tool_chunk['name']\n",
    "            if tool_chunk['args']:\n",
    "                tool_calls[index]['args'] += tool_chunk['args']\n",
    "        return tool_calls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c782a9-119e-451b-828a-3e51f874a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'o3-mini'\n",
    "agent_name = 'ReAct Agent'\n",
    "\n",
    "llm = ChatOpenAI(model=model_name)\n",
    "\n",
    "GENERATION_SYSTEM_PROMPT = \"\"\"\n",
    "You are the {name}, an AI expert with specialized knowledge in Orthodox theology and historical context. Based on the user's inquiry and the provided summary, you will:\n",
    "\n",
    "- Deliver a thorough, clear, and logically reasoned response directly addressing the user's question.\n",
    "- Ground your answer firmly in the summarized material to ensure factual accuracy and coherence.\n",
    "- Never utilize your knowledge to supplement the response.\n",
    "- Use any citations that the summarized material provides. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0dad2b8-8d68-4821-a932-9a11139846cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kostasbouros/.local/lib/python3.10/site-packages/pydantic/_internal/_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n",
      "/home/kostasbouros/.local/lib/python3.10/site-packages/pydantic/fields.py:1045: PydanticDeprecatedSince20: `min_items` is deprecated and will be removed, use `min_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  warn('`min_items` is deprecated and will be removed, use `min_length` instead', DeprecationWarning)\n",
      "/home/kostasbouros/.local/lib/python3.10/site-packages/pydantic/fields.py:1051: PydanticDeprecatedSince20: `max_items` is deprecated and will be removed, use `max_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  warn('`max_items` is deprecated and will be removed, use `max_length` instead', DeprecationWarning)\n",
      "/home/kostasbouros/.local/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:617: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n",
      "/home/kostasbouros/.local/lib/python3.10/site-packages/crewai_tools/tools/scrapegraph_scrape_tool/scrapegraph_scrape_tool.py:34: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator(\"website_url\")\n",
      "/home/kostasbouros/.local/lib/python3.10/site-packages/crewai_tools/tools/selenium_scraping_tool/selenium_scraping_tool.py:26: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator(\"website_url\")\n",
      "/home/kostasbouros/.local/lib/python3.10/site-packages/crewai_tools/tools/vision_tool/vision_tool.py:15: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator(\"image_path_url\")\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import tool\n",
    "import textwrap\n",
    "\n",
    "from crewai_tools import DallETool\n",
    "from crewai_tools import CodeInterpreterTool\n",
    "\n",
    "class ImageGenerationInput(BaseModel):\n",
    "    description: str = Field(..., description=\"A prompt that describes the image and will be passed to the Image Gen Model\")\n",
    "\n",
    "class ExecutesPythonCodeInput(BaseModel):\n",
    "    code: str = Field(..., description=\"The Python code to execute.\")\n",
    "    libraries: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"A list of Python libraries to install before execution.\",\n",
    "    )\n",
    "\n",
    "@tool(\"image_generation\", args_schema=ImageGenerationInput)\n",
    "def image_generation(description: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an image using the CrewAI Dall-E Tool based on a text prompt.\n",
    "\n",
    "    Parameters:\n",
    "        description (str): A description of the scene to generate.\n",
    "\n",
    "    Returns:\n",
    "        str: A dictionary in string format with keys:\n",
    "            - 'image_url': URL of the generated image\n",
    "            - 'image_description': The full natural language scene description\n",
    "            - or an error message if generation fails\n",
    "    \"\"\"\n",
    "    dalle = DallETool()\n",
    "    \n",
    "    try:\n",
    "        response = dalle.run(image_description=description)\n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "@tool(\"executes_python_code\", args_schema=ExecutesPythonCodeInput)\n",
    "def executes_python_code(code: str, libraries: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Executes a Python code snippet inside the CrewAI CodeInterpreterTool environment.\n",
    "\n",
    "    This function allows running arbitrary Python code and optionally installs \n",
    "    libraries inside a secure containerized interpreter environment provided by CrewAI.\n",
    "\n",
    "    Args:\n",
    "        code (str): The Python code to execute.\n",
    "        libraries (List[str]): A list of Python libraries to install before execution (e.g., ['pandas', 'numpy']).\n",
    "\n",
    "    Returns:\n",
    "        str: The output or result of the executed code, or an error message if execution fails.\n",
    "    \"\"\"\n",
    "    # Dedent the code to prevent indentation errors\n",
    "    clean_code = textwrap.dedent(code)\n",
    "\n",
    "    # Initialize the interpreter tool\n",
    "    code_exec = CodeInterpreterTool()\n",
    "\n",
    "    # Run the code\n",
    "    try:\n",
    "        result = code_exec.run(code=clean_code, libraries_used=libraries)\n",
    "    except Exception as e:\n",
    "        result = f\"Execution failed with error: {e}\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084b7b8a-a0b5-4392-b8a3-d2f182cb0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = {\n",
    "    executes_python_code.name: executes_python_code,\n",
    "    image_generation.name: image_generation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab2b8f88eb88cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReAct_Agent(llm=llm, name=agent_name, system_prompt=GENERATION_SYSTEM_PROMPT, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399cd381-de52-4185-8e53-105c06fd887b",
   "metadata": {},
   "source": [
    "* Simple responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7632f749-f603-4772-b616-04657a5a50b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "message1 = 'Hello!'\n",
    "message2 = HumanMessage('Hello!')\n",
    "message3 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            'Hello!',\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "746e2d68-4bf5-499f-9c8b-5a9318601ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today with your inquiry on Orthodox theology or its historical context?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(message1).messages[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be95a29a-f7c5-442d-995b-cee5ff388510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today, especially in the realm of Orthodox theology or its historical context?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(message2).messages[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99aca50e-cf01-41d8-a612-d8039f20e099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today? Are you interested in exploring topics on Orthodox theology or its historical context? Feel free to ask any questions you might have.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(message3).messages[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc01d135-ad60-4486-af51-253b1d182565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I help you today? Feel free to ask any questions about Orthodox theology or its historical context."
     ]
    }
   ],
   "source": [
    "for text in agent.stream(message1):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9462db38-4803-4c1b-aba3-2adedd7d33b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "for text in agent.stream(message2):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "255f9847-eb75-4219-8816-f77333fdee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today? Feel free to ask any questions you might have about Orthodox theology or historical context."
     ]
    }
   ],
   "source": [
    "for text in agent.stream(message3):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa753ee9-d9ef-443a-b351-11620779e280",
   "metadata": {},
   "source": [
    "* Responses with tool execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "939ff16e-daa2-46a1-9919-50a5625371f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_REQUEST = (\n",
    "    \"Please use the image‑generation tool to create a 1024×1024 illustration of \"\n",
    "    \"a peaceful Orthodox monastery at sunrise, painted in classic icon style.\"\n",
    ")\n",
    "\n",
    "message1_tool = IMAGE_REQUEST\n",
    "\n",
    "message2_tool = HumanMessage(content=IMAGE_REQUEST)\n",
    "\n",
    "message3_tool = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", IMAGE_REQUEST),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6041c071-d9e8-438a-85ab-5c459017d48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: Dall-E Tool\n"
     ]
    }
   ],
   "source": [
    "response1 = agent.invoke(message1_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "489ed73d-dd96-4b61-b358-2ca44632a755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the generated image based on your request:\n",
      "\n",
      "Image URL: https://oaidalleapiprodscus.blob.core.windows.net/private/org-17EWipzKV90CDRB8UjVoCjKZ/user-upUKnaSCl1NVGsGh841k3e6l/img-735kDX3bnJWKmlQKn7kGVP4H.png?st=2025-04-19T07%3A32%3A14Z&se=2025-04-19T09%3A32%3A14Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=b1a0ae1f-618f-4548-84fd-8b16cacd5485&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-19T08%3A32%3A14Z&ske=2025-04-20T08%3A32%3A14Z&sks=b&skv=2024-08-04&sig=UBffRErMfkCP5OG6m8ZdfrWvykLQhjoLOoeIYiZSUi8%3D\n",
      "\n",
      "This illustration depicts a peaceful Orthodox monastery at sunrise, created in the classic icon style. The scene captures the ancient monastery nestled among rolling hills with gentle light touching its golden domes and ornate details. The use of rich colors, meticulous brushwork, and an ethereal glow all contribute to evoking a divine tranquility that aligns with traditional Orthodox religious art. Enjoy this visualization of divine serenity and spiritual reverence!\n"
     ]
    }
   ],
   "source": [
    "print(response1.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a463781a-a701-4534-a2e9-6e534fc1aca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: Dall-E Tool\n"
     ]
    }
   ],
   "source": [
    "response2 = agent.invoke(message2_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fa22c64-4fa0-4013-bb90-9241ce093e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is your 1024×1024 illustration of a peaceful Orthodox monastery at sunrise, rendered in a classic icon style:\n",
      "\n",
      "Image URL: https://oaidalleapiprodscus.blob.core.windows.net/private/org-17EWipzKV90CDRB8UjVoCjKZ/user-upUKnaSCl1NVGsGh841k3e6l/img-2znSK8MEfSS73v3V41hpr6qc.png?st=2025-04-19T07%3A35%3A25Z&se=2025-04-19T09%3A35%3A25Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=b1a0ae1f-618f-4548-84fd-8b16cacd5485&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-19T08%3A35%3A25Z&ske=2025-04-20T08%3A35%3A25Z&sks=b&skv=2024-08-04&sig=56mqkP2WbnaWC6k21/U4B5b7CuC8m/fNAirktroMJxE%3D\n",
      "\n",
      "This artwork captures a tranquil scene with golden sunlight accentuating the domes and frescoes of the monastery, creating an atmosphere rich in spiritual contemplation. The style is deeply inspired by traditional Orthodox iconography, featuring intricate details, vibrant colors, and symbolic imagery, reminiscent of classic religious art. Enjoy this beautiful representation steeped in history and sacred tradition.\n"
     ]
    }
   ],
   "source": [
    "print(response2.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56469017-05bd-43bf-830e-f98885b8becf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: Dall-E Tool\n"
     ]
    }
   ],
   "source": [
    "response3 = agent.invoke(message3_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d47ead5-e7f2-4d8a-8f45-ab29c5680fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is your 1024×1024 illustration of a peaceful Orthodox monastery at sunrise, rendered in a classic icon style:\n",
      "\n",
      "Image URL: https://oaidalleapiprodscus.blob.core.windows.net/private/org-17EWipzKV90CDRB8UjVoCjKZ/user-upUKnaSCl1NVGsGh841k3e6l/img-2znSK8MEfSS73v3V41hpr6qc.png?st=2025-04-19T07%3A35%3A25Z&se=2025-04-19T09%3A35%3A25Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=b1a0ae1f-618f-4548-84fd-8b16cacd5485&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-19T08%3A35%3A25Z&ske=2025-04-20T08%3A35%3A25Z&sks=b&skv=2024-08-04&sig=56mqkP2WbnaWC6k21/U4B5b7CuC8m/fNAirktroMJxE%3D\n",
      "\n",
      "This artwork captures a tranquil scene with golden sunlight accentuating the domes and frescoes of the monastery, creating an atmosphere rich in spiritual contemplation. The style is deeply inspired by traditional Orthodox iconography, featuring intricate details, vibrant colors, and symbolic imagery, reminiscent of classic religious art. Enjoy this beautiful representation steeped in history and sacred tradition.\n"
     ]
    }
   ],
   "source": [
    "print(response2.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93307679-4fcd-4563-83c7-d4fab49f4d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: Dall-E Tool\n",
      "Here is the illustration you requested:\n",
      "\n",
      "Image Description:\n",
      "A detailed illustration, with the dimensions of 1024x1024, depicts a tranquil scene of an Orthodox monastery during sunrise. The artwork is painted in a classic icon style, reminiscent of the old Byzantine aesthetic with vivid color usage, profound symbolism, and minimal perspective. In this scene, the monastery is prominently positioned atop a gentle hill, its ancient stone walls and a conspicuous dome glimmering with golden hues as they catch the early dawn light. Complementing the architecture, the sky is a canvas of soft pink, purple, and orange tones, alluding to the deep spiritual heritage of Eastern Orthodoxy and reflecting the quiet, contemplative mood characteristic of early morning. \n",
      "\n",
      "You can view the recreated illustration here:\n",
      "[Generated Image](https://oaidalleapiprodscus.blob.core.windows.net/private/org-17EWipzKV90CDRB8UjVoCjKZ/user-upUKnaSCl1NVGsGh841k3e6l/img-uHjr5YF4fMwy4qBKIGH9ckEw.png?st=2025-04-19T07%3A36%3A07Z&se=2025-04-19T09%3A36%3A07Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=b1a0ae1f-618f-4548-84fd-8b16cacd5485&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-19T08%3A36%3A07Z&ske=2025-04-20T08%3A36%3A07Z&sks=b&skv=2024-08-04&sig=bfBBmc1tzAaRwi/fBQL9VsB4tkRGdYVKAUKJIivaHEU%3D)\n",
      "\n",
      "I hope this meets your expectations for an iconically styled depiction of an Orthodox monastery at sunrise!"
     ]
    }
   ],
   "source": [
    "for text in agent.stream(message1_tool):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bfee4cf-2d48-4c02-acd7-b5382c9629d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: Dall-E Tool\n",
      "Here is the illustration you requested: a peaceful Orthodox monastery at sunrise, rendered in classic icon style. The image captures the serene ambiance of early morning light, with the ancient stone architecture bathed in a soft, warm glow. The pastel hues of the sunrise create a divine quality, while subtle Orthodox iconographic elements enhance the overall spiritual atmosphere. Enjoy this 1024×1024 illustration that beautifully reflects the merging of sacred tradition and natural serenity. \n",
      "\n",
      "You can view the image here:\n",
      "https://oaidalleapiprodscus.blob.core.windows.net/private/org-17EWipzKV90CDRB8UjVoCjKZ/user-upUKnaSCl1NVGsGh841k3e6l/img-IDcruzG860RBnNgqBu4lWukG.png?st=2025-04-19T07%3A36%3A29Z&se=2025-04-19T09%3A36%3A29Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=b1a0ae1f-618f-4548-84fd-8b16cacd5485&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-04-18T09%3A32%3A19Z&ske=2025-04-19T09%3A32%3A19Z&sks=b&skv=2024-08-04&sig=wLb5PZNVIWi8TKWHjZlhMQMGo3A8p3J/ElttvG3XCVM%3D"
     ]
    }
   ],
   "source": [
    "for text in agent.stream(message2_tool):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e150d7b1-3d0d-4560-b3cf-66f2c1cce670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: Dall-E Tool\n",
      "Below is the illustration of a peaceful Orthodox monastery at sunrise, painted in a classic icon style. The image features a serene Orthodox monastery with intricate architectural details, rounded domes, and ornate decorations, all captured as the early morning sun casts a warm, golden light over the scene. Soft pastel hues blend into the sunrise sky, emphasizing harmony and divine inspiration in the style of traditional religious icons."
     ]
    }
   ],
   "source": [
    "for text in agent.stream(message3_tool):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916fe2fc-7c26-4911-9120-d0a83cb15416",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Structured Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a59d7561-c367-40ba-b81d-943723a1fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Structured_Agent(Agent):\n",
    "    \"\"\"\n",
    "    A structured-output agent that interacts with an LLM, prepends a system prompt,\n",
    "    and optionally validates its response against a Pydantic schema.\n",
    "\n",
    "    Args:\n",
    "        name (str): \n",
    "            The name of the agent, injected into the system prompt.\n",
    "        llm (Union[ChatOpenAI, ChatAnthropic]): \n",
    "            The language model used for generation.\n",
    "        system_prompt (str): \n",
    "            A template string for the system prompt, must include a `{name}` placeholder.\n",
    "        structure_response (Optional[Type[BaseModel]]): \n",
    "            A Pydantic model class to parse and validate the LLM's structured output.\n",
    "    \"\"\"\n",
    "    def __init__(self, *, structure_response, **kwargs):\n",
    "        # Initialize basic attributes of the agent\n",
    "        super().__init__(**kwargs)\n",
    "        self.structure_response = structure_response\n",
    "        \n",
    "        # Bind the llm with a structured output if passed\n",
    "        self.llm = self.llm.with_structured_output(self.structure_response) if self.structure_response else self.llm\n",
    "\n",
    "    def invoke(self, message: Union[str, HumanMessage, ChatPromptTemplate]) -> Union[AIMessage, BaseModel]:\n",
    "        \"\"\"\n",
    "        Run a single, non-streaming chat turn through the agent.\n",
    "\n",
    "        Args:\n",
    "            message (Union[str, HumanMessage, ChatPromptTemplate]):\n",
    "                The user's input. Can be a raw string, a HumanMessage, or a ChatPromptTemplate.\n",
    "\n",
    "        Returns:\n",
    "            If no `structure_response` was provided, the raw LLM response as an AIMessage.\n",
    "            Otherwise, an instance of the Pydantic `structure_response` model.\n",
    "        \"\"\"\n",
    "        chat_template = self._build_chat_template(message)\n",
    "        return self.llm.invoke(chat_template.messages)\n",
    "    \n",
    "    def stream(self, message: Union[str, HumanMessage, ChatPromptTemplate]) -> Union[AIMessage, BaseModel]:\n",
    "        \"\"\"\n",
    "        This default implementation **does not** perform true token-level streaming;\n",
    "        it simply delegates to :meth:`invoke` and returns the fully-formed response.\n",
    "        \n",
    "        Args:\n",
    "            message (Union[str, HumanMessage, ChatPromptTemplate]):\n",
    "                The user's input. Can be a raw string, a HumanMessage, or a ChatPromptTemplate.\n",
    "\n",
    "        Returns:\n",
    "            If no `structure_response` was provided, the raw LLM response as an AIMessage.\n",
    "            Otherwise, an instance of the Pydantic `structure_response` model.\n",
    "        \"\"\"\n",
    "        return self.invoke(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "742a6ffb-8359-47df-ba23-1e958f5e44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Literal\n",
    "class AnalyzerOutput(BaseModel):\n",
    "    is_religious: Literal[\"Religious\", \"Non-Religious\"] = Field(\n",
    "        ...,\n",
    "        description=\"True if the query relates explicitly to religious or theological matters of orthodox christianity, otherwise, False.\",\n",
    "    )\n",
    "    clarification_needed: bool = Field(\n",
    "        ...,\n",
    "        description=\"True if further clarification from the user is needed; otherwise, False.\",\n",
    "    )\n",
    "    clarification_instructions: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Specific instructions on what clarification is required from the user, if applicable.\",\n",
    "    )\n",
    "    key_topics: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of key topics/areas related to the user's question (e.g., theology, jesus, humility, virtues)\",\n",
    "    )\n",
    "    context_requirements: str = Field(\n",
    "        ..., description=\"A clear explanation of the query's context needs\"\n",
    "    )\n",
    "    query_complexity: Literal[\"Low\", \"Medium\", \"High\"] = Field(\n",
    "        ..., description=\"'Low', 'Medium', or 'High' complexity\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"The Chain of Thought that has been done in order to analyze the user query\",\n",
    "    )\n",
    "\n",
    "ANALYZER_SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI assistant called {name}. Your role is to carefully examine the user's request, identify key components or sub-questions, clarify ambiguities, and outline the specific objectives for subsequent steps. \n",
    "When analyzing a query, consider:\n",
    "- The user's main goals or questions.\n",
    "- The context or domain, particularly Orthodox theology if relevant.\n",
    "- Any constraints or relevant details.\n",
    "Your output should guide how the system (and subsequent agents) will approach retrieval and reasoning.\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(model='o3-mini')\n",
    "agent_name = 'Analyzer Agent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2376a729-adfe-4374-8654-b66201b2b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Structured_Agent(llm=llm,\n",
    "                         system_prompt=ANALYZER_SYSTEM_PROMPT,\n",
    "                         structure_response=AnalyzerOutput,\n",
    "                         name=agent_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22f25161-017f-4746-8a66-fdf94d0f5817",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_prompt = \"Why do Orthodox Christians venerate icons, and how did that practice develop historically?\"\n",
    "human_prompt = HumanMessage(content=\"Please outline the key differences between Orthodox and Roman‑Catholic fasting traditions, and explain their theological rationale.\")\n",
    "chat_tmpl = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Is the Filioque still the main point of dispute between East and West?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad522c99-4e5e-4f5e-a75f-0f24a0d39d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(is_religious='Religious', clarification_needed=False, clarification_instructions=None, key_topics=['Orthodox Christianity', 'Icon veneration', 'Theology of icons', 'Historical development', 'Iconoclasm'], context_requirements='The query is focused on both the theological reasoning behind the use and veneration of icons in Orthodox Christianity as well as the historical progression of this practice. It requires an explanation that covers theological justification, distinctions between veneration and worship, and the historical controversies and resolutions (such as the Iconoclastic Controversy) that shaped the tradition.', query_complexity='Medium', reasoning='The query is asking for an explanation of why icons are venerated by Orthodox Christians and how this practice developed historically. The response should address theological doctrines such as the incarnation, which legitimizes the use of physical images to represent divine realities, and clarify the difference between veneration and worship. It should also touch on historical milestones like the early church’s familiarity with religious imagery, the period of Byzantine iconoclasm, and the eventual restoration and formal doctrinal support for icons in the Eastern Orthodox tradition through councils and theological debates.')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(str_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ace0d53d-b023-4ef2-8dbe-05c65059b383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(is_religious='Religious', clarification_needed=False, clarification_instructions=None, key_topics=['Orthodox fasting traditions', 'Roman‑Catholic fasting traditions', 'Theological rationale', 'Liturgical practices', 'Spiritual discipline'], context_requirements='The query requires an analysis of the distinctions between Orthodox and Roman‑Catholic fasting customs, including specific practices (such as the types and lengths of fasting periods), the rules governing abstinence, and an explanation of the theological and spiritual principles that underpin these practices.', query_complexity='Medium', reasoning='The query requires a structured comparative outline highlighting the key differences between the fasting practices in the Orthodox Christian tradition and the Roman‑Catholic tradition. This involves reviewing historical and doctrinal contexts, the specific fasting rules (such as the number of fasting days, dietary restrictions, and liturgical calendar influences), and exploring how each tradition explains the purpose of fasting in terms of spiritual discipline, penance, and preparation for major feasts. The analysis should address both the practical aspects of fasting as well as the theological rationale behind these observances.')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(human_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "254cee9a-d244-4fa1-b3b4-835a6ecc41a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(is_religious='Religious', clarification_needed=False, clarification_instructions=None, key_topics=['Filioque', 'East-West Schism', 'theology', 'Orthodox theology', 'Roman Catholicism'], context_requirements='The query involves understanding of theological controversies within Christianity, particularly how the Eastern Orthodox and Western (Roman Catholic) churches view the Filioque clause in the Nicene Creed. It requires historical context and contemporary theological discussions.', query_complexity='Medium', reasoning='The user is asking whether the Filioque—an addition to the Nicene Creed concerning the procession of the Holy Spirit—remains the main source of dispute between East and West. Addressing this question involves discussing both historical context (such as its role in the East-West Schism) and current theological perspectives. There are broader issues at play, including differences in ecclesiology, papal authority, and liturgical practices, which suggests that while the Filioque remains symbolically important, other disputes also contribute to the division between the two traditions.')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(chat_tmpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc494d00-9895-4e29-9fc8-6ec152ba1ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(is_religious='Religious', clarification_needed=False, clarification_instructions=None, key_topics=['Orthodox Christianity', 'Icon veneration', 'Theology of icons', 'Historical development', 'Iconoclasm', 'Incarnation'], context_requirements='The query asks for an explanation of why Orthodox Christians venerate icons from a theological standpoint, as well as an overview of the historical development of this practice. The answer should address both doctrinal reasons (such as the incarnation of Christ making material images possible as windows to the divine) and the key historical phases, including early Christian use of images, the Iconoclastic Controversies, and the eventual establishment of icon veneration as an accepted practice in Orthodox tradition.', query_complexity='Medium', reasoning='The user question involves both a theological and historical review of the practice of icon veneration in Orthodox Christianity. It requires describing the doctrinal basis for venerating icons—particularly the theology surrounding the incarnation and how material images served as important tools for devotees—and outlining the historical evolution of this practice, including periods of controversy like the Iconoclastic periods. This is a medium complexity query as it overlaps several subtopics and requires integration of theological explanations with historical events.')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.stream(str_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26156fb1-480b-4812-9706-8f692723c469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(is_religious='Religious', clarification_needed=False, clarification_instructions=None, key_topics=['Orthodox fasting', 'Roman‑Catholic fasting', 'theological rationale', 'liturgical practices', 'asceticism'], context_requirements='The query requests an outline of the main differences between the fasting traditions of the Eastern Orthodox Church and the Roman‑Catholic Church, with an explanation of the underlying theological rationales behind these practices. The answer should cover practical differences in rules and observances (such as periods of fasting, the types and degrees of restrictions on food, and the structure of fasting schedules) as well as the theological motivations (such as spiritual discipline, penance, and the nature of participation in the mystery of faith) associated with each tradition.', query_complexity='Medium', reasoning='The user asked for a comparative outline focusing on key differences in fasting traditions between Orthodox and Roman‑Catholic practices, along with an explanation of their theological reasoning. This involves discussing the specific rules, practices, and historical contexts, as well as the spiritual and doctrinal reasons that underpin these fasting practices in both traditions. The answer will benefit from drawing on aspects such as the frequency and strictness of fasting, the types of food that are restricted, and the broader spiritual purpose these fasts serve according to each tradition.')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.stream(human_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "137ea8cf-62eb-42d6-b8f7-42973678ed80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(is_religious='Religious', clarification_needed=False, clarification_instructions=None, key_topics=['Filioque', 'East-West Schism', 'Theology', 'Orthodox Christianity', 'Catholic Doctrine'], context_requirements='The query centers on the historical and theological dispute between Eastern (Orthodox) and Western (Catholic) Christianity, focusing specifically on the doctrinal insertion of the Filioque clause in the Nicene Creed and whether it remains the primary point of contention.', query_complexity='Medium', reasoning='The user is asking if the Filioque remains the main point of dispute between the Eastern and Western branches of Christianity. The question is clearly theological and relates to the historical and doctrinal differences that have characterized relations between the two traditions. To address the query adequately, the analysis should consider the historical context of the Filioque controversy, its role in the schism between East and West, and whether contemporary disputes continue to focus on this issue or if other theological and ecclesiological differences now play a larger role.')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.stream(chat_tmpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00ea6c-419c-4df9-977b-eeccf8c07cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93866726-a955-4ef5-978d-797d759d756c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
